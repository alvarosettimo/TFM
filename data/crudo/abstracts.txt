This randomized clinical trial investigates the effect of a machine learning recommender messaging system and viral peer recruitment tool kit on a smoking-cessation intervention.

========================================

Background and Objectives
Nursing homes (NHs) using the Preferences for Everyday Living Inventory (PELI-NH) to assess important preferences and provide person-centered care find the number of items (72) to be a barrier to using the assessment.


Research Design and Methods
Using a sample of n = 255 NH resident responses to the PELI-NH, we used the 16 preference items from the MDS 3.0 Section F to develop a machine learning recommender system to identify additional PELI-NH items that may be important to specific residents. Much like the Netflix recommender system, our system is based on the concept of collaborative filtering whereby insights and predictions (e.g., filters) are created using the interests and preferences of many users. The algorithm identifies multiple sets of "you might also like" patterns called association rules, based upon responses to the 16 MDS preferences that recommends an additional set of preferences with a high likelihood of being important to a specific resident.


Results
In the evaluation of the combined apriori and logistic regression approach, we obtained a high recall performance (i.e., the ratio of correctly predicted preferences compared with all predicted preferences and nonpreferences) and high precision (i.e., the ratio of correctly predicted rules with respect to the rules predicted to be true) of 80.2% and 79.2%, respectively.


Discussion and Implications
The recommender system successfully provides guidance on how to best tailor the preference items asked of residents and can support preference capture in busy clinical environments, contributing to the feasibility of delivering person-centered care.

========================================

Changing and moving toward online shopping has made it necessary to
customize customers’ needs and provide them more selective options. The
buyers search the products’ features before deciding to purchase items.
The recommender systems facilitate the searching task for customers via
narrowing down the search space within the specific products that align
the customer needs. Clustering, as a typical machine learning approach,
is applied in recommender systems. As an information filtering method, a
recommender system clusters user’s data to indicate the required factors
for more accurate predictions by calculating the similarity between
members of a cluster. In this study, using the Gaussian mixture model
clustering and considering the scores distance and the value of scores
in the Pearson correlation coefficient, a new method is introduced for
predicting scores in machine learning recommender systems. To study the
proposed method’s performance, a Movie Lens data set is evaluated, and
the results are compared to some other recommender systems, including
the Pearson correlation coefficients similarity criteria, K-means, and
fuzzy C-means algorithms. The simulation results indicate that our
method has less error than others by increasing the number of neighbors.
The results also illustrate that when the number of users increases, the
proposed method’s accuracy will increase. The reason is that the
Gaussian mixture clustering chooses similar users and considers the
scores distance in choosing similar neighbors to the active user.

========================================

Saudi Arabia’s tourism sector has recently started to play a significant role as an economic driver. The restaurant industry in Riyadh has experienced rapid growth in recent years, making it increasingly challenging for customers to choose from the large number of restaurants available. This paper proposes a matrix factorization collaborative-based recommender system for Riyadh city restaurants. The system leverages user reviews and ratings to predict users’ preferences and recommend restaurants likely to be of interest to them. The system incorporates three different approaches, namely, non-negative matrix factorization (NMF), singular value decomposition (SVD), and optimized singular value decomposition (SVD++). To the best of our knowledge, this is the first recommender system specifically designed for Riyadh restaurants. A comprehensive dataset of restaurants in Riyadh was collected, scraped from Foursquare.com, which includes a wide range of restaurant features and attributes. The dataset is publicly available, enabling other researchers to replicate the experiments and build upon the work. The performance of the system was evaluated using a real-world dataset, and its effectiveness was demonstrated by comparing it to a state-of-the-art recommender system. The evaluation results showed that SVD and NMF are effective methods for generating recommendations, with SVD performing slightly better in terms of RMSE and NMF performing slightly better in terms of MAE. Overall, the findings suggest that the collaborative-based approach using matrix factorization algorithms is an effective way to capture the complex relationships between users and restaurants.

========================================

Machine learning has proven its efficacy in solving agricultural problems in the recent years such as crop recommendation, crop yield prediction, and many such. With the advancement in the sub-domain of machine learning i.e., deep learning, multiple problems are minutely solved in agricultural sector. This paper focuses on recommending 22 types of crops with the aid of correlation analysis, distribution analysis, ensembling, and majority voting. A three-tiered framework is proposed in order to implement the crop recommendation problem. It includes data preprocessing, classification, and performance evaluation modules. The feature analysis is done through correlation plots and density distribution followed by classification using ensembling techniques. Finally, performance evaluation is performed using majority voting technique. This article further uses ensembling with base learners i.e., decision trees, random forest, Naïve Bayes, and support vector machines using majority voting. Further, majority voting is used to decide the final performance metrics. The practical visualization of the correlation plot, density-histogram distribution plots, confusion matrices, and performance plot are presented. The accuracy achieved post implementation is 99.54& by using Naïve Bayes classifier. The majority voting ensembler has not shown much accuracy i.e., 98.52&. Thus, Naïve Bayes classifier is proved to be the best fit for this problem statement. Some challenges and future research directions are also epitomized in this article.

========================================

Agriculture and its allied sectors are undoubtedly the largest providers of livelihoods in rural India. The agriculture sector is also a significant contributor factor to the country’s Gross Domestic Product (GDP). Blessing to the country is the overwhelming size of the agricultural sector. However, regrettable is the yield per hectare of crops in comparison to international standards. This is one of the possible causes for a higher suicide rate among marginal farmers in India. This paper proposes a viable and user-friendly yield prediction system for the farmers. The proposed system provides connectivity to farmers via a mobile application. GPS helps to identify the user location. The user provides the area & soil type as input. Machine learning algorithms allow choosing the most profitable crop list or predicting the crop yield for a user-selected crop. To predict the crop yield, selected Machine Learning algorithms such as Support Vector Machine (SVM), Artificial Neural Network (ANN), Random Forest (RF), Multivariate Linear Regression (MLR), and K-Nearest Neighbour (KNN) are used. Among them, the Random Forest showed the best results with 95% accuracy. Additionally, the system also suggests the best time to use the fertilizers to boost up the yield.

========================================

Nowadays, social networks have become highly relevant in the professional field, in terms of the possibility of sharing profiles, skills and jobs. LinkedIn has become the social network par excellence, owing to its content in professional and training information and where there are also endorsements, which are validations of the skills of users that can be taken into account in the recruitment process, as well as in the recommender system. In order to determine how endorsements influence Lifelong Learning course recommendations for professional skills development and enhancement, a new version of our Lifelong Learning course recommendation system is proposed. The recommender system is based on ontology, which allows modelling the data of knowledge areas and job performance sectors to represent professional skills of users obtained from social networks. Machine learning techniques are applied to group entities in the ontology and make predictions of new data. The recommender system has a semantic core, content-based filtering, and heuristics to perform the formative suggestion. In order to validate the data model and test the recommender system, information was obtained from web-based lifelong learning courses and information was collected from LinkedIn professional profiles, incorporating the skills endorsements into the user profile. All possible settings of the system were tested. The best result was obtained in the setting based on the spatial clustering algorithm based on the density of noisy applications. An accuracy of 94% and 80% recall was obtained.

========================================

Web mining procedure helps the surfers to get the required information but finding the exact information is as good as finding a needle in a haystack. In this work, an intelligent prediction model using Tensor Flow environment for Graphics Processing Unit (GPU) devices has been designed to meet the challenges of speed and accuracy. The proposed approach is isolated into two stages: pre-processing and prediction. In the first phase, the procedure starts via looking through the URLs of various e-learning sites particular to computer science subjects. At that point, the content of looked through URLs are perused and after that from their keywords are produced identified with a particular subject in the wake of playing out the pre-processing of the content. Second phase is prediction that predicts query specific links of e-learning website. The proposed Intelligent E-learning through Web (IEW) has content mining, lexical analysis, classification and machine learning based prediction as its key features. Algorithms like SVM, Naïve Bayes, K-Nearest Neighbor, and Random Forest were tested and it was found that Random Forest gave an accuracy of 98.98%, SVM 42%, KNN 63% and Naïve Bayes 66%. Based on the results IEW uses Random forest for prediction.

========================================

Abstract Recommender systems use different techniques of machine learning (ML) to suggest users and recommend service or entity in various field of application such as in health care recommender system (HRS). Due to the vast count of algorithms shown in the literature, HRS and various application sectors are now utilizing ML algorithms from the area of artificial intelligence. However, selecting an appropriate ML algorithm in the case of a health recommender system seems to be a time-consuming task. However the development of recommender system in different service domain faces problems of algorithms selection for better accuracy. This article examined the usage of ML techniques in recommender systems for health applications through a survey of the literature. The objectives of this article are (i) recognize the literature review finding of recommender system in health applications using ML and deep learning algorithms. (ii) Assist new researchers with the help of gap in previous research. The results of this study is to proposed new recommender system in health application of mosquito borne disease by using hybrid approach of ML technique.

========================================

: As an integral component of human society, higher education has been undergoing a transformation in multiple aspects, such as administrative reorganization, pedagogical reform, and technological innovation. To line up with the latest trends, many institutions constantly update their curriculum, which poses challenges to students and their advisors. This paper proposes a machine learning-based course enrollment recommender system that aims to make personalized suggestions to students who expect to take classes in the upcoming semester. Using matrix factorization as the core algorithm, the model exploits several available types of information, including student course enrollment history and other contextual features, such as prerequisite restrictions, course meeting times, instructional methods, and course instructors. The system not only helps students but also facilitates their advisors’ work. Our experimental results show that the recommended courses were highly relevant while providing plenty of options to students.

========================================

In the 21st century, University educations are becoming a key pillar of social and economic life. It plays a major role not only in the educational process but also in the ensuring of two important things which are a prosperous career and financial security. However, predicting university admission can be especially difficult because the students are not aware of admission requirements. For that reason, the main purpose of this research work is to provide a recommender system for early predicting university admission based on four Machine Learning algorithms namely Linear Regression, Decision Tree, Support Vector Regression, and Random Forest Regression. The experimental results showed that the Random Forest Regression is the most suitable Machine Learning algorithm for predicting university admission. Also, the Cumulative Grade Point Average (CGPA) is the most important parameter that influences the chance of admission.

========================================

In recent years, with the rapid growth of Internet technology, online shopping has become a rapid way for users to purchase and consume desired products. Large volume of user-generated content on social media sites like twitter has resulted in tweet sentiment analysis. Sentiment analysis supports as base for decision support systems and recommendation systems and it becomes an essential tool on online platforms to extract the information on user emotional state to improve user satisfaction. This paper proposes an effective sentiment analysis recommender system framework using machine learning models.

========================================

Outcome-based education (OBE) is a well-proven teaching strategy based upon a predefined set of expected outcomes. The components of OBE are Program Educational Objectives (PEOs), Program Outcomes (POs), and Course Outcomes (COs). These latter are assessed at the end of each course and several recommended actions can be proposed by faculty members’ to enhance the quality of courses and therefore the overall educational program. Considering a large number of courses and the faculty members’ devotion, bad actions could be recommended and therefore undesirable and inappropriate decisions may occur. In this paper, a recommender system, using different machine learning algorithms, is proposed for predicting suitable actions based on course specifications, academic records, and course learning outcomes’ assessments. We formulated the problem as a multi-label multi-class binary classification problem and the dataset was translated into different problem transformation and adaptive methods such as one-vs.-all, binary relevance, label powerset, classifier chain, and ML-KNN adaptive classifier. As a case study, the proposed recommender system is applied to the college of Computer and Information Sciences, Jouf University, Kingdom of Saudi Arabia (KSA) for helping academic staff improving the quality of teaching strategies. The obtained results showed that the proposed recommender system presents more recommended actions for improving students’ learning experiences.

========================================

The recommender system is the most profound research area for e-commerce product recommendations. Currently, many e-commerce platforms use a text-based product search, which has limitations to fetch the most similar products. An image-based similarity search for recommendations had considerable gains in popularity for many areas, especially for the e-commerce platforms giving a better visual search experience by the users. In our research work, we proposed a machine-learning-based approach for a similar image-based recommender system. We applied a dimensionality reduction technique using Principal Component Analysis (PCA) through Singular Value Decomposition (SVD) for transforming the extracted features into lower-dimensional space. Further, we applied the K-Means++ clustering approach for the possible cluster identification for a similar group of products. Later, we computed the Manhattan distance measure for the input image to the target clusters set for fetching the top-N similar products with low distance measure. We compared our approach with five different unsupervised clustering algorithms, namely Minibatch, K-Mediod, Agglomerative, Brich, and the Gaussian Mixture Model (GMM), and used the 40,000 fashion product image dataset from the Kaggle web platform for the product recommendation process. We computed various cluster performance metrics on K-means++ and achieved a Silhouette Coefficient (SC) of 0.1414, a Calinski-Harabasz (CH) index score of 669.4, and a Davies–Bouldin (DB) index score of 1.8538. Finally, our proposed PCA-SVD transformed K-mean++ approach showed superior performance compared to the other five clustering approaches for similar image product recommendations.

========================================

The exponential growth of recommender systems research has drawn the attention of the scientific community recently. These systems are very useful in reducing information overload and providing users with the items of their need. The major areas where recommender systems have contributed significantly include e-commerce, online auction, and books and conference recommendation for academia and industrialists. Book recommender systems suggest books of interest to users according to their preferences and requirements. In this article, we have surveyed machine learning techniques which have been used in book recommender systems. Moreover, evaluation metrics applied to evaluate recommendation techniques is also studied. Six categories for book recommendation techniques have been identified and discussed which would enable the scientific community to lay a foundation of research in the concerned field. We have also proposed future perspectives to improve recommender system. We hope that researchers exploring recommendation technology in general and book recommendation in particular will be finding this work highly beneficial.

========================================

This research aims to determine the similarities in groups of people to build a film recommender system for users. Users often have difficulty in finding suitable movies due to the increasing amount of movie information. The recommender system is very useful for helping customers choose a preferred movie with the existing features. In this study, the recommender system development is established by using several algorithms to obtain groupings, such as the K-Means algorithm, birch algorithm, mini-batch K-Means algorithm, mean-shift algorithm, affinity propagation algorithm, agglomerative clustering algorithm, and spectral clustering algorithm. We propose methods optimizing K so that each cluster may not significantly increase variance. We are limited to using groupings based on Genre and Tags for movies. This research can discover better methods for evaluating clustering algorithms. To verify the quality of the recommender system, we adopted the mean square error (MSE), such as the Dunn Matrix and Cluster Validity Indices, and social network analysis (SNA), such as Degree Centrality, Closeness Centrality, and Betweenness Centrality. We also used average similarity, computational time, association rule with Apriori algorithm, and clustering performance evaluation as evaluation measures to compare method performance of recommender systems using Silhouette Coefficient, Calinski-Harabaz Index, and Davies–Bouldin Index.

========================================

In the past decade, recommender systems have become an essential part of online services such as NetFlix, YouTube, online shopping, etc. The tourism agencies such as TripAdvisor or Expedia also apply the recommender system to their services. For Thailand, the tourism industry is one of the most important revenues of the country. The problem is that the recommender system for planning a trip to Thailand still not effective enough. Users require a lot of effort when planning a trip. Therefore, the objective of this study is to develop the prototype of a tourism recommender system that automatically understands the user's preferences of their favorite tourist attractions without asking them any question. It applied machine learning to extract the user's preferences from the user's Instagram photos. Those preferences then use to compute the similarity with the attributes from 23 example tourist attractions in Ubon Ratchathani Province. A user study was conducted with 42 participates to preliminary study the precision and the adoption of the prototype. The results suggested that the prototype has been judged as satisfactory by participants for both precision and adoption. Moreover, the findings of this study will serve as insights for the direction of our planned future research such as applying the recommender system to other provinces of Thailand.

========================================

A University admission process is a complex process that needs a tremendous amount of time and labor to allocate course(s) to the prospective applicants. The study aimed at tackling the wrong placement of applicants into courses and to also address the wastage of admission vacancies by recommending the appropriate course(s). About 8,700 data for three academic sessions were collected from two different universities for training and testing the system. The features used are the results of Senior Secondary School subjects and the average score of UTME and Post-UTME. The proposed system employed five (5) classification models, which include Linear Regression, Naive Bayes, Support Vector Machine, K-Nearest Neighbor and Decision Tree Algorithm. The Linear Regression Model has the Root Mean Square Error (RMSE) as 2. 614 $e^{-14}$ The other four (4) classification models are also found to be efficient with an efficiency of at least 90% sequel to the dimensionality reduction in the dataset. The result shows that both Naive Bayes Classifier and Support Vector Machine have achieved the highest recommendation accuracy of approximately 99.94%, which outperformed Decision Tree and K-Nearest Neighbor algorithms with an accuracy of 98.10% and 99.87% respectively. The system can be adjusted with the change of admission criteria in Nigerian Universities. In consideration of the high degree of prediction accuracy, flexibility is an advantage, as the system can predict suitable courses that match the students’ grades. Therefore, the system is adaptive and good in making the prediction and can be used as a framework for further research on the admission recommender system.

========================================

Allocation of courses and research students based on faculty’s subject specialization and area of interest has always remained a challenging task for university administration due to the presence of academics’ cross-domain interests, stale faculty resumes at university portals and changing the skill set demands from the industry. Collaborative filtering and content-based recommender systems have already been in use by the industry for recommending things, such as movies, news, restaurants, and shopping items to the users, and however, no one has utilized these off-the-shelf models for enhancing the student experience and improving the quality of higher education in academia. This paper presents a case study showcasing the use of probabilistic topic models for generating recommendations to users in academia through appropriate course allocation and supervisor assignment. The proposed system coined as ScholarLite harnesses the power of machine learning to extract research themes from faculty members’ past publications, mines research interests from their resumes, and combines it with their educational background to generate recommendations for course teaching, research supervision, and industry–academia collaboration. We have shown the recommendation results on real-world data gathered from the higher education commission of the country and demonstrated that the proposed techniques are scalable across various programs offered by the universities and could be deployed in a small budget by universities for automating course and supervisor allocation procedures. The experiments confirm our performance expectation by showing good relevance and objectivity in results, thus making this decision management system more appealing for large-scale deployment and use by academia.

========================================

Recommender systems based on sentiment analysis become challenging due to the presence of enormous data available over the internet. With the lack of proper data cleaning and analysis methods, existing machine learning (ML) techniques fail to generate accurate recommendations. To overcome this issue, this paper proposes a Light Deep Learning (LightDL)-based recommender system that uses Twitter-based reviews. First, the data is collected from Twitter and cleaned by subsequent data cleaning processes. Then, this pre-processed data is fed into the LightDL model, which learns the important features like hashtags, unigrams, multigrams, etc. from each piece of data. Here, we have learned about four groups of features, including semantic features, syntactic features, symbolic features, and tweet-based features. Finally, the data is classified into positive, negative, and neutral categories according to the learned features. On the basis of classified sentiment, the review is generated to the users. Finally, the model is evaluated in terms of accuracy, precision, recall, f-measure, and error rate through extensive experiments in Matlab. The proposed LightDL model outperforms in all performance measures; specifically, it achieves 95% accuracy for the Twitter dataset.

========================================

In the information-overloaded era of the Web, recommender systems that provide personalized content filtering are now the mainstream portal for users to access Web information. Recommender systems deploy machine learning models to learn users’ preferences from collected historical data, leading to more centralized recommendation results due to the feedback loop. As a result, it will harm the ranking of content outside the narrowed scope and limit the options seen by users. In this work, we first conduct data analysis from a graph view to observe that the users’ feedback is restricted to limited items, verifying the phenomenon of centralized recommendation. We further develop a general simulation framework to derive the procedure of the recommender system, including data collection, model learning, and item exposure, which forms a loop. To address the filter bubble issue under the feedback loop, we then propose a general and easy-to-use reinforcement learning-based method, which can adaptively select few but effective connections between nodes from different communities as the exposure list. We conduct extensive experiments in the simulation framework based on large-scale real-world datasets. The results demonstrate that our proposed reinforcement learning-based control method can serve as an effective solution to alleviate the filter bubble and the separated communities induced by it. We believe the proposed framework of controllable recommendation in this work can inspire not only the researchers of recommender systems, but also a broader community concerned with artificial intelligence algorithms’ impact on humanity, especially for those vulnerable populations on the Web.

========================================

Machine learning forms the base of many information retrieval applications those effect our day to day lives directly or indirectly. One of the Commonly used application of machine learning algorithms is Recommender Systems. Recommender system are information flitering system which takes users rating for items into account and predict user preferences. Many online ecommerce and other categorical websites are able to generate recommendations either on the basis of implicit feedback or explicit feedback. In implicit feedback, preferences are actually based on analysis of browsing patterns of the user, for example, purchase history, web logs etc. Explicit feedback is generated from the ratings provided by the user. In this paper we have shown adaption of collaborative filtering in Apache Mahout platforms via Eclipse on a sample data set.

========================================

Autoencoders have become a hot researched topic in unsupervised learning due to their ability to learn data features and act as a dimensionality reduction method. With rapid evolution of autoencoder methods, there has yet to be a complete study that provides a full autoencoders roadmap for both stimulating technical improvements and orienting research newbies to autoencoders. In this paper, we present a comprehensive survey of autoencoders, starting with an explanation of the principle of conventional autoencoder and their primary development process. We then provide a taxonomy of autoencoders based on their structures and principles and thoroughly analyze and discuss the related models. Furthermore, we review the applications of autoencoders in various fields, including machine vision, natural language processing, complex network, recommender system, speech process, anomaly detection, and others. Lastly, we summarize the limitations of current autoencoder algorithms and discuss the future directions of the field.

========================================

e-Learning is a sought-after option for learners during pandemic situations. In e-Learning platforms, there are many courses available, and the user needs to select the best option for them. Thus, recommender systems play an important role to provide better automation services to users in making course choices. It makes recommendations for users in selecting the desired option based on their preferences. This system can use machine intelligence (MI)-based techniques to carry out the recommendation mechanism. Based on the preferences and history, this system is able to know what the users like most. In this work, a recommender system is proposed using the collaborative filtering mechanism for e-Learning course recommendation. This work is focused on MI-based models such as K-nearest neighbor (KNN), Singular Value Decomposition (SVD) and neural network–based collaborative filtering (NCF) models. Here, one lakh of Coursera’s course review dataset is taken from Kaggle for analysis. The proposed work can help learners to select the e-Learning courses as per their preferences. This work is implemented using Python language. The performance of these models is evaluated using performance metrics such as hit rate (HR), average reciprocal hit ranking (ARHR) and mean absolute error (MAE). From the results, it is observed that KNN is able to perform better in terms of higher HR and ARHR and lower MAE values as compared to other models.

========================================

Social Media platforms are already an indispensable part of our daily lives. With its constant growth, it has contributed to superfluous, heterogeneous data which can be overwhelming due to its volume and velocity, thus limiting the availability of relevant and required information when a particular query is to be served. Hence, a need for personalized, fine-grained user preference-oriented framework for resolving this problem and also, to enhance user experience is increasingly felt. In this paper, we propose a such a social framework, which extracts user's reviews, comments of restaurants and points of interest such as events and locations, to personalize and rank suggestions based on user preferences. Machine Learning and Sentiment Analysis based techniques are used for further optimizing search query results. This provides the user with quicker and more relevant data, thus avoiding irrelevant data and providing much needed personalization.

========================================

This paper presents a machine learning approach to predict the amount of compute memory needed by jobs which are submitted to Load Sharing Facility (LSF® ) with a high level of accuracy. LSF® is the compute resource manager and job scheduler for Qualcomm chip design process. It schedules the jobs based on available resources: CPU, memory, storage, and software licenses. Memory is one of the key resources and its proper utilization leads to a substantial improvement in saving machine resources which in turn results in a significant reduction in overall job pending time. In addition, efficient memory utilization helps to reduce the operations cost by decreasing the number of servers needed for the end-to-end design process. In this paper, we explored a suite of statistical and machine learning techniques to develop a Compute Memory Recommender System for the Qualcomm chip design process with over 90% accuracy in predicting the amount of memory a job needs. Moreover, it demonstrates the potential to significantly reduce job pending time.

========================================

Deep recommender systems (DRS) are critical for current commercial online service providers, which address the issue of information overload by recommending items that are tailored to the user’s interests and preferences. They have unprecedented feature representations effectiveness and the capacity of modeling the non-linear relationships be-tween users and items. Despite their advancements, DRS models, like other deep learning models, employ sophisticated neural network architectures and other vital components that are typically designed and tuned by human experts. This article will give a comprehensive summary of automated machine learning (AutoML) for developing DRS models. We ﬁrst provide an overview of AutoML for DRS models and the related techniques. Then we discuss the state-of-the-art AutoML approaches that automate the feature selection, feature embeddings, feature interactions, and system design in DRS. Finally, we discuss appealing research directions and summarize the survey.

========================================

: E-commerce is the most essential application for conducting business transactions. Delivering product information to customers require an essential machine called recommender system. Recommender systems have been adopted in many large e-commerce companies such as Amazon, e-Bay, Alibaba, YouTube, iTunes, and so on. Ratings have become an essential factor to calculate product information. They are users’ expressions about their satisfaction regarding a product or service. Unfortunately, the number of ratings is extremely sparse. Generating rating prediction is a major issue in the recommender system research field. The most popular model using latent factor or matrix factorization to generate rating prediction faced the problem in accuracy performance. This research aimed to develop a novel model to generate rating prediction using two deep learning variants based on Stack Denoising Auto Encoder (SDAE), Long Short Term Memory (LSTM), and combining with a latent factor model based on Probabilistic Matrix Factorization (PMF). This study considered integrated information resources, including user information and document product information. Following the experiment report involved in Movielens and Amazon Information Video dataset, our model outperformed previous works using PMF, Collaborative Deep Learning (CDL), Probabilistic Hybrid Deep Learning (PHD) and LSTM-PMF model, with more than 5% in average using Root Mean Square Error (RMSE) evaluation metrices.

========================================

Due to aggressive urbanization (with population size), waste increases exponentially, resulting in environmental damage. Even though it looks challenging, such an issue can be controlled if we can reuse them. To handle this, in our work, we design a machine learning and blockchain-oriented system that identifies the waste objects/products and recommends to the user multiple ‘Do-It-Yourself’ (DIY) ideas to reuse or recycle. Blockchain records every transaction in the shared ledger to enable transaction verifiability and supports better decision-making. In this study, a Deep Neural Network (DNN) trained on about 11700 images is developed using ResNet50 architecture for object recognition (training accuracy of 94%). We deploy several smart contracts in the Hyperledger Fabric (HF) blockchain platform to validate recommended DIY ideas by blockchain network members. HF is a decentralized ledger technology platform that executes the deployed smart contracts in a secured Docker container to initialize and manage the ledger state. The complete model is delivered on a web platform using Flask, where our recommendation system works on a web scraping script written using Python. Fetching DIY ideas using web-scraping takes nearly 1 second on a desktop machine with an Intel Core-i7 processor with 8 cores, 16 GB RAM, installed with Ubuntu 18.04 64-bit operating system, and Python 3.6 package. Further, we evaluate blockchain-based smart contracts’ latencies and throughput performances using the hyperledger caliper benchmark. To the best of our knowledge, this is the first work that integrates blockchain technology and deep learning for the DIY recommender system.

========================================

Failure management and cost-aware traffic engineering are two important tasks done in Network Operation Centers (NOC). These are performed by expert technicians who must carefully analyze the network state and the flow of incoming alarms to decide how, where and when to take actions on the network. While based on implicit guiding principles, these network actions are very hard to automate with explicit rules due to the high complexity of the system; hence NOC action is essentially a manual process today. To automate part of that process, in this paper we introduce an Action Recommendation Engine (ARE) that can learn implicit NOC action rules with supervised machine learning from historical data. As a result, ARE can recommend suitable action(s) to remedy network faults and engineer the traffic to minimize costs, all while maximizing the users’ Quality of Experience. To quantify the effectiveness of different NOC action scenarios, we introduce the QoE-OPEX metric which balances between users’ quality of Experience and ISP’s operational costs. After proper model training on 56,000 data points with 66 features, we demonstrate that ARE can effectively reproduce implicit action-taking logic of NOC technicians, thus moving us one step closer to reliable autonomous networks and fully-automated NOCs.

========================================

Recommender systems offer several advantages to hospital data management units and patients with special needs. These systems are more dependent on the extreme subtle hospital-patient data. Thus, disregarding the confidentiality of patients with special needs is not an option. In recent times, several proposed techniques failed to cryptographically guarantee the data privacy of the patients with special needs in the diet recommender systems (RSs) deployment. In order to tackle this pitfall, this paper incorporates a blockchain privacy system (BPS) into deep learning for a diet recommendation system for patients with special needs. Our proposed technique allows patients to get notifications about recommended treatments and medications based on their personalized data without revealing their confidential information. Additionally, the paper implemented machine and deep learning algorithms such as RNN, Logistic Regression, MLP, etc., on an Internet of Medical Things (IoMT) dataset acquired via the internet and hospitals that comprises the data of 50 patients with 13 features of various diseases and 1,000 products. The product section has a set of eight features. The IoMT data features were analyzed with BPS and further encoded prior to the application of deep and machine learning-based frameworks. The performance of the different machine and deep learning methods were carried out and the results verify that the long short-term memory (LSTM) technique is more effective than other schemes regarding prediction accuracy, precision, F1-measures, and recall in a secured blockchain privacy system. Results showed that 97.74% accuracy utilizing the LSTM deep learning model was attained. The precision of 98%, recall, and F1-measure of 99% each for the allowed class was also attained. For the disallowed class, the scores were 89, 73, and 80% for precision, recall, and F1-measure, respectively. The performance of our proposed BPS is subdivided into two categories: the secured communication channel of the recommendation system and an enhanced deep learning approach using health base medical dataset that spontaneously identifies what food a patient with special needs should have based on their disease and certain features including gender, weight, age, etc. The proposed system is outstanding as none of the earlier revised works of literature described a recommender system of this kind.

========================================

Recommender System (RS) is one of the most popular applications of Artificial Intelligence which attracted researchers all around the world. Many machine learning algorithms are used to develop RSs. Choosing the best machine learning algorithm to provide users with a product or service is the most challenging task in the area of RSs. Now we are witnessing a paradigm shift in the purchase habits of people from in-shop to online resulting in the availability of online information exponentially growing every day. The ever-increasing online information and the number of online users create new avenues in RS. In an online shopping scenario, these systems must be able to recommend relevant items to the users. The RSs have to deal with the huge amount of information by filtering the relevant information based on the analysis made on the inputs made by the users during their online sessions. These systems can recommend appropriate items to users based on their interest and previous preference which can lead to increased sales. The three major techniques used to build a RS are content-based, collaborative based and hybrid-based. This paper presents the various applications of RSs and makes a detailed comparative study of different machine learning approaches used. The methodologies used for identifying research articles for analysis, the merits and demerits of different techniques in RSs and domain-specific applications of these techniques are well explained here with scientific review analysis.

========================================

Recommender systems (RS) are ubiquitous in digital space. This paper develops a deep learning-based approach to address three practical challenges in RS: complex structures of high-dimensional data, noise in relational information, and the black-box nature of machine learning algorithms. Our method—Multi-GraphGraph Attention Network (MG-GAT)—learns latent user and business representations by aggregating a diverse set of information from neighbors of each user (business) on a neighbor importance graph. MG-GAT out-performs state-of-the-art deep learning models in the recommendation task using two large-scale datasets collected from Yelp and four other standard datasets in RS. The improved performance highlights MG-GAT’s advantage in incorporating multi-modal features in a principled manner. The features importance, neighbor importance graph, and latent representations reveal business insights on predictive features and explainable characteristics of business and users. Moreover, the learned neighbor importance graph can be used in a variety of management applications, such as targeting customers, promoting new businesses, and designing information acquisition strategies. Our paper presents a quintessential big data application of deep learning models in management while providing interpretability essential for real-world decision-making.

========================================

Recommender systems have become extremely important to various types of industries where customer interaction and feedback is paramount to the success of the business. For companies that face changes that arise with ever‐growing markets, providing product recommendations to new and existing customers is a challenge. Our goal is to give our customers personalized recommendations based on what other similar people with similar portfolios have, in order to make sure they are adequately covered for their needs. Our system uses customer characteristics in addition to customer portfolio data. Since the number of possible recommendable products is relatively small, compared to other recommender domains, and missing data is relatively frequent, we chose to use Bayesian Networks for modeling our systems. We also present a deep‐learning‐based approach to provide recommendations to prospects (potential customers) where only external marketing data is available at the time of prediction.

========================================

Since coronavirus has shown up, inaccessibility of legitimate clinical resources is at its peak, like the shortage of specialists and healthcare workers, lack of proper equipment and medicines etc. The entire medical fraternity is in distress, which results in numerous individual's demise. Due to unavailability, individuals started taking medication independently without appropriate consultation, making the health condition worse than usual. As of late, machine learning has been valuable in numerous applications, and there is an increase in innovative work for automation. This paper intends to present a drug recommender system that can drastically reduce specialists heap. In this research, we build a medicine recommendation system that uses patient reviews to predict the sentiment using various vectorization processes like Bow, TF-IDF, Word2Vec, and Manual Feature Analysis, which can help recommend the top drug for a given disease by different classification algorithms. The predicted sentiments were evaluated by precision, recall, f1score, accuracy, and AUC score. The results show that classifier LinearSVC using TF-IDF vectorization outperforms all other models with 93% accuracy.

========================================

Recommender systems algorithms are generally evaluated primarily on machine learning criteria such as recommendation accuracy or top-n precision. In this work, we evaluate six recommendation algorithms from a user-centric perspective, collecting both objective user activity data and subjective user perceptions. In a field experiment involving 1508 users who participated for at least a month, we compare six algorithms built using machine learning techniques, ranging from supervised matrix factorization, contextual bandit learning to Q learning. We found that the objective design in machine-learning-based recommender systems significantly affects user experience. Specifically, a recommender optimizing for implicit action prediction error engages users more than optimizing for explicit rating prediction error when modeled with the classical matrix factorization algorithms, which empirically explains the historical transition of recommender system research from modeling explicit feedback data to implicit feedback data. However, the action-based recommender is not as precise as the rating-based recommender in that it increases not only positive engagement but also negative engagement, e.g., negative action rate and user browsing effort which are negatively correlated with user satisfaction. We show that blending both explicit and implicit feedback from users through an online learning algorithm can gain the benefits of engagement and mitigate one of the possible costs (i.e., the increased browsing effort).

========================================

Web development without an integrated structure makes lots of difficulties for users. Web personalization systems are presented to make the website compatible with interest of users in both aspects of contents and services. In this paper extracting user navigation patterns is used to capture similar behaviors of users in order to increase the quality of recommendations. Based on patterns extracted from the same user navigation, recommendations are provided to the user to make it easier to navigate. Recently, web browsing techniques have been widely used for personalization. In this study, a method is proposed to create a user profile with the web usage mining by clustering and neural networks in order to predict the user's future requests and then generate a list of the pages of user's favorites. Simulation results shows that proposed method will increase the accuracy of recommender systems.

========================================

Conventional recommender systems are required to train the recommendation model using a centralized database. However, due to data privacy concerns, this is often impractical when multi-parties are involved in recommender system training. Federated learning appears as an excellent solution to the data isolation and privacy problem. Recently, Graph neural network (GNN) is becoming a promising approach for federated recommender systems. However, a key challenge is to conduct embedding propagation while preserving the privacy of the graph structure. Few studies have been conducted on the federated GNN-based recommender system. Our study proposes the first vertical federated GNN-based recommender system, called VerFedGNN. We design a framework to transmit: (i) the summation of neighbor embeddings using random projection, and (ii) gradients of public parameter perturbed by ternary quantization mechanism. Empirical studies show that VerFedGNN has competitive prediction accuracy with existing privacy preserving GNN frameworks while enhanced privacy protection for users' interaction information.

========================================

Recent studies have shown that robust diets recommended to patients by Dietician or an Artificial Intelligent automated medical diet based cloud system can increase longevity, protect against further disease, and improve the overall quality of life. However, medical personnel are yet to fully understand patient-dietician’s rationale of recommender system. This paper proposes a deep learning solution for health base medical dataset that automatically detects which food should be given to which patient base on the disease and other features like age, gender, weight, calories, protein, fat, sodium, fiber, cholesterol. This research framework is focused on implementing both machine and deep learning algorithms like, logistic regression, naive bayes, Recurrent Neural Network (RNN), Multilayer Perceptron (MLP), Gated Recurrent Units (GRU), and Long Short-Term Memory (LSTM). The medical dataset collected through the internet and hospitals consists of 30 patient’s data with 13 features of different diseases and 1000 products. Product section has 8 features set. The features of these IoMT data were analyzed and further encoded before applying deep and machine and learning-based protocols. The performance of various machine learning and deep learning techniques was carried and the result proves that LSTM technique performs better than other scheme with respect to forecasting accuracy, recall, precision, and <inline-formula> <tex-math notation="LaTeX">$F1$ </tex-math></inline-formula>-measures. We achieved 97.74% accuracy using LSTM deep learning model. Similarly 98% precision, 99% recall and <inline-formula> <tex-math notation="LaTeX">$99\%~F1$ </tex-math></inline-formula>-measure for allowed class is achieved, and for not-allowed class precision is 89%, recall score is 73% and <inline-formula> <tex-math notation="LaTeX">$F1$ </tex-math></inline-formula> Measure score is 80%.

========================================

A decision-making system is one of the most important tools in data mining. The data mining field has become a forum where it is necessary to utilize users' interactions, decision-making processes and overall experience. Nowadays, e-learning is indeed a progressive method to provide online education in long-lasting terms, contrasting to the customary head-to-head process of educating with culture. Through e-learning, an ever-increasing number of learners have profited from different programs. Notwithstanding, the highly assorted variety of the students on the internet presents new difficulties to the conservative one-estimate fit-all learning systems, in which a solitary arrangement of learning assets is specified to the learners. The problems and limitations in well-known recommender systems are much variations in the expected absolute error, consuming more query processing time, and providing less accuracy in the final recommendation. The main objectives of this research are the design and analysis of a new transductive support vector machine-based hybrid personalized hybrid recommender for the machine learning public data sets. The learning experience has been achieved through the habits of the learners. This research designs some of the new strategies that are experimented with to improve the performance of a hybrid recommender. The modified one-source denoising approach is designed to preprocess the learner dataset. The modified anarchic society optimization strategy is designed to improve the performance measurements. The enhanced and generalized sequential pattern strategy is proposed to mine the sequential pattern of learners. The enhanced transductive support vector machine is developed to evaluate the extracted habits and interests. These new strategies analyze the confidential rate of learners and provide the best recommendation to the learners. The proposed generalized model is simulated on public datasets for machine learning such as movies, music, books, food, merchandise, healthcare, dating, scholarly paper, and open university learning recommendation. The experimental analysis concludes that the enhanced clustering strategy discovers clusters that are based on random size. The proposed recommendation strategies achieve better significant performance over the methods in terms of expected absolute error, accuracy, ranking score, recall, and precision measurements. The accuracy of the proposed datasets lies between 82 and 98%. The MAE metric lies between 5 and 19.2% for the simulated public datasets. The simulation results prove the proposed generalized recommender has a great strength to improve the quality and performance.

========================================

Federated Learning (FL) is a popular distributed machine learning paradigm that enables devices to work together to train a centralized model without transmitting raw data. However, when the model becomes complex, mobile devices’ communication overhead can be unacceptably large in traditional FL methods. To address this problem, Federated Distillation (FD) is proposed as a federated version of knowledge distillation. Most of the recent FD methods calculate the model output (logits) of each client as the local knowledge on a public proxy dataset and do distillation with the average of the clients’ logits on the server side. Nevertheless, these FD methods are not robust and perform poorly in the non-IID (data is nonindependent and non-identically distributed) scenario such as Federated Recommendation (FR). In order to eliminate the non-IID problem and apply FD in FR, we proposed a novel method named FedDyn to construct a proxy dataset and extract local knowledge dynamically in this paper. In this method, we replaced the average strategy with focus distillation to strengthen reliable knowledge, which solved the non-IID problem that the local model has biased knowledge. The average strategy is a dilution and perturbation of knowledge since it treats reliable and unreliable knowledge equally important. In addition, to prevent inference of private user information from local knowledge, we used a method like local differential privacy techniques to protect this knowledge on the client side. The experimental results showed that our method has a faster convergence speed and lower communication overhead than the baselines on three datasets, including MovieLens-10OK, MovieLens-IM and Pinterest.

========================================

Multi-modal transport recommender systems aim to provide different users with different route choices for more than one mode of transportation. Most existing systems focus on unimodal transportation providing shortest distance or travel time. Knowing that the use of machine learning and deep learning techniques are achieving success in many fields, it has also been applied to improve the transport networks by helping individuals to meet their needs and observe their various preferences. In this paper, we develop a model called MTRecS-DLT (Multi-Modal Transport Recommender System using Deep Learning and Tree Models) for recommending the most appropriate transport mode for different users. We have used the weighted average ensembling method of Convolutional Neural Network (CNN) and Gradient-Boosted Decision Trees (GBDT) that shows promising results. We have extracted context and user features from the training data. Then, CNN has been applied to extract latent features. The proposed model utilizes a weighted average ensembling to combine CNN and GBDT.

========================================

Programming education has recently received increased attention due to growing demands for programming and information technology skills. However, a lack of teaching materials and human resources presents a major challenge to meeting the growing demand for programming education. One way to compensate for a shortage of trained teachers is to use machine learning techniques to assist learners. Therefore, we propose a learning path recommendation system based on a learner’s ability charts by means of a recurrent neural network. In brief, a learning path is constructed from a learner’s submission history with a trial-and-error process, and the learner’s ability chart is used as a barometer of their current knowledge. In this paper, an approach for constructing a learning path recommendation system by using ability charts and its implementation based on a sequential prediction model by a recurrent neural network, are presented. Experimental evaluation with data from an e-learning system is also provided.

========================================

In the context of intelligent digital learning, we propose an agent-based recommender system that aims to help learners overcome their gaps by suggesting relevant learning resources. The main idea is to provide them with appropriate support in order to make their learning experience more effective. To this end we design an agent-based cooperative system where autonomous agents are able to update recommendation data and to improve the recommender outcome on behalf of their past experiences in the learning platform.

========================================

While recent years have witnessed a rapid growth of research papers on recommender system (RS), most of the papers focus on inventing machine learning models to better fit user behavior data. However, user behavior data is observational rather than experimental. This makes various biases widely exist in the data, including but not limited to selection bias, position bias, exposure bias, and popularity bias. Blindly fitting the data without considering the inherent biases will result in many serious issues, e.g., the discrepancy between offline evaluation and online metrics, hurting user satisfaction and trust on the recommendation service, and so on. To transform the large volume of research models into practical improvements, it is highly urgent to explore the impacts of the biases and perform debiasing when necessary. When reviewing the papers that consider biases in RS, we find that, to our surprise, the studies are rather fragmented and lack a systematic organization. The terminology “bias” is widely used in the literature, but its definition is usually vague and even inconsistent across papers. This motivates us to provide a systematic survey of existing work on RS biases. In this paper, we first summarize seven types of biases in recommendation, along with their definitions and characteristics. We then provide a taxonomy to position and organize the existing work on recommendation debiasing. Finally, we identify some open challenges and envision some future directions, with the hope of inspiring more research work on this important yet less investigated topic. The summary of debiasing methods reviewed in this survey can be found at https://github.com/jiawei-chen/RecDebiasing.

========================================

Recommender system is a new generation of internet tool that helps users to access the web and receive information about their preferences. Using an online recommender is comparatively an easy and faster procedure to purchase items and this is done quickly. Recommendation systems plays an indispensable role in ecommerce websites to help users in identifying the right goods. One of the best methods to increase profits and attract customers is a recommendation process. The existing methodologies allow the systems to collect the irrelevant data and lead to a downfall in attracting the users and completing their work in a quick and reliable way. This paper provides an overview of the Recommendation Systems that is currently employed in the operations of the online book shopping domain. This paper proposes a simple understandable system for book recommendations that help readers to suggest the right book, which is to be studied next. In recent years, information analysis challenge has been focused on for the administration recommendation system. For clients, network assets are completely linked and quickly developed. The proposed method works on training, feedback, management, reporting, configuration, and using it to offer useful information to the user in order to aid in decision-making and data item recommendations. We have used a User Based Collaborative Filtering (UBCF) approach and measured the performance of similarity measures in recommending books to a user. The proposed system’s overall architecture is introduced and its implementation is represented with a model design.

========================================

Information overload is one of the potential setbacks to many e-commerce platform users. It is very important to filter the media and the choices that are overwhelming for internet users while making buying decisions using online stores. To solve this problem, recommendation systems are used widely. A recommender system helps users find a product of their own choice by filtering and prioritizing and effectively generating the relevant information to its users. The purpose of a recommender system is to save time and hassle of searching through the World Wide Web, instead it generates specific and relevant content that promotes online transaction and bring satisfaction to the users of e-commerce platforms. The proposed system is an e-commerce platform based on an apparel recommendation system that recommends products on the foundation of the user's preferences.

========================================

Nowadays a big challenge when going out to a new restaurant or cafe, people usually use websites or applications to look up nearby places and then choose one based on an average rating. But most of the time the average rating isn't enough to predict the quality or hygiene of the restaurant. Different people have different perspectives and priorities when evaluating a restaurant. Many online businesses now have implemented personalized recommendation systems which basically try to identify user preferences and then provide relevant products to enhance the users experience . In turn, users will be able to enjoy exploring what they might like with convenience and ease because of the recommendation results. Finding an ideal restaurant can be a struggle because the mainstream recommender apps have not yet adopted the personalized recommender approach. So we took up this challenge and we aim to build the prototype of a personalized recommender system that incorporates metadata which is basically the information provided by interactions of customers and restaurants online(reviews), which gives a pretty good idea of customers satisfaction and taste as well as features of the restaurant. This type of approach enhances user experience of finding a restaurant that suits their taste better. This paper has used a package called lightfm(the library of python for implementing popular recommendation algorithms) and the dataset from yelp. There are different methods of filtering the data, here we have used Hybrid filtering which is a combination of Content-based filtering (CBF) and Collaborative Filtering (CF). Since the results from Hybrid filtering are far more closer to accuracy than CBF or CF respectively. Then hybrid filtering gives results in the form of personalized recommendations for users after training and testing of the data

========================================

Recommender systems add significant benefits to E-commerce in terms of sale conversion, revenues, customer experience, loyalty and lifetime value. But the recommendations from these systems do not change on inputs beyond user and item profile and transaction data. There have been some attempts in the past to optimize on more varied data in recommenders, example of which is the location based recommenders. But location is just one dimension of the state that a user could have shared with GPS/GLONASS/BaiDeu sensor available in most Smartphones. With an upcoming era of Smart-wears and pervasive IoTs, there are a lot many other dimensions of a user state which can be utilized to optimize upon the concept of Optimal State Recommender Solutions. This paper suggests upgrading from conventional recommendations that are based on user/ item preferences alone with systems that provide the best recommendation at the most optimal state when the user is most receptive to accept the recommendation, the “optimal state recommendation solution” and proposes solutions and architectures to overcome the challenges of dealing with real time, distributed machine learning on IoT scale data in implementing this solution. The paper leverages some of the advance distributed machine learning algorithms like variants of Distributed Kalman Filters, Distributed Alternating Least Square Recommenders, Distributed Mini-Batch Stochastic Gradient Descent(SGD) based Classifiers, and highly scalable distributed computation and machine learning platforms like Apache Spark, (Apache) Spark MLlib, Spark Streaming, Python/PySpark, R/SparkR, Apache Kafka in an high performance, distributed, fault tolerant architecture. The solution also aspires to be compliant with upcoming IoT standards and architectures like IEEE P2413 to provide a standard solution for such problems beyond the current scope of this paper.

========================================

In today’s highly competitive job market, it is becoming increasingly important for companies to hire employees who are best fit for a job and to ensure they retain these employees in the long run. Studies have shown that employees who find their job meaningful and satisfying are generally more productive and less likely to leave the job. Human Resource professionals therefore need to ensure that proper screening of candidates is conducted during the recruitment process and that they hire the best fit candidate for a job. Given the usually high number of applicants for a particular job, the recruitment process is time consuming and it is not always possible to conduct proper screening and interviews for each applicant. This paper presents the development of JobFit, a job recommendation system which makes use of a recommender system, machine learning techniques and past data to predict the best fit candidate for a job. The proposed job recommendation system takes as input the requirement of a job and the profile of the applicants and outputs a JobFit score indicating how fit each applicant is for the particular job. The system ultimately provides the HR professionals with a sorted list of all candidates with those who are more fit and apt for the job recommended first. This shall help to ensure the HR focus on the screening and interviews of only a small pool of candidates, the best ones recommended by the system, while at the same time be confident that the better candidates are not being missed.

========================================

Recommender systems with the approach of collaborative filtering by using the algorithms of machine learning gives better optimized results. But selecting the appropriate learning rate and regularized parameter is not an easy task. RMSE changes from one set of these values to others. The best set of these parameters has to be selected so that the RMSE must be optimized. In this paper we proposed a method to resolve this problem. Our proposed system selects appropriate learning rate and regularized parameter for given data.

========================================

Every year new movies are released with a varied story-line or a genre which could be of potential interest to viewers. Various online movie or video streaming platforms can keep the customers engaged by recommending movies of the viewer’s preference. A key research challenge for Recommender engines is make more targeted recommendations. This paper presents Filtering approaches including Content-based, which recommends items (movies) to the user (viewer) based on their previous history/ preferences and Collaborative-based which uses opinions and actions of other similar users (viewers) to recommend items (movies). In Collaborative filtering, User-based, Item based, SVD, and SVD++ algorithms have been implemented and the performance evaluated. Finally, a hybrid recommendation engine that stacks both the Content-based and SVD filtering models is shown to have optimal performance and improved movie recommendations to retain active viewer engagement with the service.

========================================

Offline evaluation is an essential complement to online experiments in the selection, improvement, tuning, and deployment of recommender systems. Offline methodologies for recommender system evaluation evolved from experimental practice in Machine Learning (ML) and Information Retrieval (IR). However, evaluating recommendations involves particularities that pose challenges to the assumptions upon which the ML and IR methodologies were developed. We recap and reflect on the development and current status of recommender system evaluation, providing an updated perspective. With a focus on offline evaluation, we review the adaptation of IR principles, procedures and metrics, and the implications of those techniques when applied to recommender systems. At the same time, we identify the singularities of recommendation that require different responses, or involve specific new needs. In addition, we provide an overview of important choices in the configuration of experiments that require particular care and understanding; discuss broader perspectives of evaluation such as recommendation value beyond accuracy; and survey open challenges such as experimental biases, and the cyclic dimension of recommendation.

========================================

This article removes the recommender structure for undergrad and graduate understudies which can help with picking the best schools matching their profile. The proposed model has used different extracting techniques for scrapping the data based on student profiles who have secured the seat successfully earlier. Then, machine learning technology is used to calculate the weighted scores based upon the training and testing data. This research study has introduced the KNN and Feature weighted algorithms to display the top N comparable clients for the test clients and recommend the Top M colleges to clients from the N comparative clients. As there is a colossal course of action of data and User profile, this research work is highly intended to use Knowledge-based techniques for two unmistakable models. Case-based information recommendation is used to calculate Graduate recommendations and constant-based recommendation is used for Undergraduate proposals.

========================================

Due to the growth of IoT applications, especially health care, the information of patients’ health records using data collection from IoT-connected devices has been considered. Biological data of patients in the health record helps to monitor the patient’s status and identify various diseases. Chronic diseases are a type of silent disease that, if not diagnosed in time, can cause irreparable damage to patients. The use of patients’ medical record data for early diagnosis of chronic diseases has recently attracted the attention of many researchers. On the other hand, the application of machine learning methods in the form of recommender systems has taken an important step in improving medical services and health care. In this paper, a medical recommender system was presented to identify and treat chronic diseases using an IoT device. In the present method, the electronic patient health record dataset that is loaded in the PhysioNet data repository has been used. In the present dataset, patients’ health records have been recorded according to the identified diseases and the physician’s diagnosis. In the proposed method, the 
 
 K
 
 -nearest neighbor classification method is used to identify the type of disease, and the collaborative filtering method is used to find the appropriate treatment for patients. The results of the implementation of the proposed method show that this approach, based on the use of symptom similarity among patients, has good accuracy in diagnosing and predicting chronic diseases and has provided higher results than previous methods.

========================================

As e-commerce offers more and more choices for users, its structure becomes more and more complicated. Inevitably, it brings about the problem of information overload. The solution to this problem is an e-commerce personalized recommendation system using machine learning technology. People often seem confused when facing extensive information and cannot grasp the key points. This paper studies the personalized recommendation technology of e-commerce: deeply analyzes the related technologies and algorithms of the e-commerce recommendation system and proposes the latest architecture of the e-commerce recommendation system according to the current development status of the e-commerce recommendation system. The system recommends accuracy and real-time requirements and divides the system into two parts: offline mining and online recommendation and analyzes and implements the functions and technologies of each part. User-based recommender systems, collaborative filtering recommender systems, and content-based recommender systems are analyzed, respectively. The personalized recommendation cannot only quickly help customers find the required commodity information in a wide range of complex information but also can compare more commodity information to help customers to judge. However, the existing recommendation system has some problems such as the lack of recommendation personality, the reduced relevance of recommendation, and the poor timeliness of recommendation. Finally, a recommendation system that combines three recommendation algorithms is designed, and experiments are carried out. The newly designed recommendation system is compared with three different recommendation systems, and a summary and outlook are made. Based on the introduction of the relevant theories, characteristics, and mainstream technologies of personalized recommendation based on machine learning, this document presents a constructive example of a model based on the factors that influence personalized e-commerce information recommendations in the retail sector. Through questionnaire surveys, we analyze and design the influencing factors for consumers to purchase personalized products after the survey and build a project using state-of-the-art field learning techniques. Through the model to test the eight hypotheses proposed in this paper, the results show that customer income level, customer online shopping experience, commodity prices, product quality, recommendation relevance, credit evaluation, and service quality will have a significant positive impact on shopping willingness and ultimately affect the customer’s shopping behavior. e-commerce platform can use this influencing factor to establish personalized information recommendation service mode.

========================================

Hypertension is becoming a serious health issue in the world. People tend to have a busy lifestyle and to adopt unhealthy diets. Due to poor eating habits, the rate of Non Communicable Diseases (NCDs) such as hypertension together with the rate of death caused by such diseases are rising. In order to promote healthy eating habits in Mauritius, the paper proposes a DASH diet recommender system that recommends healthy Mauritian diet plans to hypertensive patients. The system consists of a recommendation engine that uses techniques such as content-based filtering along with machine learning algorithms to recommend personalized diet plans to hypertensive patients based on factors such as age, user preferences about food, allergies, smoking level, alcohol level, blood pressure level and dietary intake. The system makes use of a mobile application which is handy and quick to use. Based on a survey carried out, the application has helped users to control and reduce their BP level.

========================================

Nowadays, while modeling environments provide users with facilities to specify different kinds of artifacts, e.g., metamodels, models, and transformations, the possibility of learning from previous modeling experiences and being assisted during modeling tasks remains largely unexplored. In this paper, we propose NEMO, a recommender system based on an Encoder-Decoder neural network to assist modelers in performing model editing operations. NEMO learns from past modeling activities and performs predictions employing a deep learning technique. Such an algorithm has been successfully applied in machine translation to convert a text from a language to another foreign language and vice versa. An empirical evaluation on a dataset of BPMN change-based persistent model demonstrates that the technique permits learning from existing operations and effectively predicting the next editing operations with considerably high prediction accuracy. In particular, NEMO gets 0.977 as precision/recall and 0.992 as success rate score by the best performance.

========================================

One of the most effective and widely used applications of machine learning technologies in business is recommender systems. Machine learning algorithms from the field of artificial intelligence have recently been used in these systems. Many varieties of movies have become increasingly diverse as technology and entertainment development have advanced, leaving users perplexed as to how to choose amongst them. The main aim of a movie recommendation system is to deal with a recommender approach using data clustering and computational intelligence. This recommender system’s goal is to acquire insights based on the user ‘s choices and to achieve customer satisfaction by presenting effective findings and optimizing the time a user spends on the website or platform.

========================================

A recommender system based on experimental databases is useful for the efficient discovery of inorganic compounds. Here, we review studies on the discovery of as-yet-unknown compounds using recommender systems. The first method used compositional descriptors made up of elemental features. Chemical compositions registered in the inorganic crystal structure database (ICSD) were supplied to machine learning for binary classification. The other method did not use any descriptors, but a tensor decomposition technique was adopted. The predictive performance for currently unknown chemically relevant compositions (CRCs) was determined by examining their presence in other databases. According to the recommendation, synthesis experiments of two pseudo-ternary compounds with currently unknown structures were successful. Finally, a synthesis-condition recommender system was constructed by machine learning of a parallel experimental data-set collected in-house using a polymerized complex method. Recommendation scores for unexperimented conditions were then evaluated. Synthesis experiments under the targeted conditions found two yet-unknown pseudo-binary oxides.

========================================

Video-game players generate huge amounts of data, as everything they do within a game is recorded. In particular, among all the stored actions and behaviors, there is information on the in-game purchases of virtual products. Such information is of critical importance in modern free-to-play titles, where gamers can select or buy a profusion of items during the game in order to progress and fully enjoy their experience. To try to maximize these kind of purchases, one can use a recommendation system so as to present players with items that might be interesting for them. Such systems can better achieve their goal by employing machine learning algorithms that are able to predict the rating of an item or product by a particular user. In this paper we evaluate and compare two of these algorithms, an ensemble-based model (extremely randomized trees) and a deep neural network, both of which are promising candidates for operational video-game recommender engines. Item recommenders can help developers improve the game. But, more importantly, it should be possible to integrate them into the game, so that users automatically get personalized recommendations while playing. The presented models are not only able to meet this challenge, providing accurate predictions of the items that a particular player will find attractive, but also sufficiently fast and robust to be used in operational settings.

========================================

Crop Recommendation System for agriculture is based on various input parameters. This paper proposes a hybrid model for recommending crops to south Indian states by considering various attributes. The recommender model is built as a hybrid model using the classifier algorithm such as Naive Bayes, J48 and association rules. Based on the appropriate parameters, the system will recommend the crop. Technologybased crop recommendation system for agriculture helps the farmers to increase the crop yield by recommending a suitable crop for their land with the help of geographic and the climatic parameters. The proposed hybrid recommender model is found to be effective in recommending a suitable crop

========================================

This paper presents the result of Systematic Literature Review (SLR) on Recommender System (RS) topic as a preliminary toward a further study on designing a smart Learning Management System (LMS) for online learning which adopts Natural Language Processing techniques. As a foundation to a broader study on smart LMS, this study focused on analyzing prominent study reports on recommender systems in general and online learning in particular. The SLR method analyzed papers published in the range of 2013-2018. Out of the 109 papers this study analyzed indepth 42 papers. The study findings confirmed that most of RS studies still focused on e-commerce, movies, tourists, and more whose most popular RS methods were collaborative filtering and content base. Some studies in RS for online education were mostly focused on scheduling, recommendations for courses, books, prospective students and others. The results of this study found that there are still much opportunities to develop methods and approaches for RS in online learning. This study findings gives foundation of our future research to develop a model of conscious contextual recommendation system using Machine Learning based on smart LMS for online learning.

========================================

The amount of content available on platforms like youtube, Netflix, Hulu, Disney is increasing each day. With the increasing number of available content, viewers started spending a lot of time searching for content of their likings. A good recommendation engine can save a lot of time for all the viewers while finding the content of their taste. A great recommender system avoids viewers wasting time watching trailers or movies for a few minutes and then switching to a different one as they did not like it. Without a recommendation system, a user might leave the platform if they cannot find a product or content of their taste. . Recommendation engines or recommender systems filter a large list of products, movies, or things to present only products or things that customers might be interested in. Recommender systems are machine learning systems that help us discover new products, media content, and services depending on their earlier activities.

========================================

Advances in Big Data analytics and machine learning have offered intangible benefits across many areas of one’s health. One such area is a move towards healthier lifestyle choices such as one’s diet. Recommender systems apply techniques that can filter information and narrow that information down based on user preferences or user needs and help users choose what information is relevant. Commonly adopted across e-commerce sites, social networking and entertainment industries, recommender systems can also support nutrition-based health management, offering individuals more food options, not only based on one’s preferred tastes but also on one’s dietary needs and restrictions. This research presents the design, implementation and evaluation of three recommender systems using content-based, collaborative filtering and hybrid recommendation models within the nutrition domain.

========================================

The advent of machine learning provides a new angle to solver different real-time challenges in business and research applications. In general, machine learning is nothing but the greater conversion of traditional mathematics application. Machine learning models play a significant role in facial recognition applications. Facial recognition is used in many real-time applications like security systems, automated attendance, offices etc. One of the important applications of machine learning models is movie recommendation using facial detection. This has been carried out by capturing the emotion to save time of the user rather than searching individual movies. Some relevant research works are carried out based on the attentional convolutional neural (recognizes each facial micro expression) and recommender system has been implemented to provide either the movies or song based on the output received from previous input i.e. CNN. Another work implemented for facial recognition using decision trees, boosting algorithms has been proven to be very inefficient compared to CNN. So, CNN seems more appropriate to obtain the best possible accuracy. Also, the combination of both the types of recommendation system i.e. content based and collaborative filtering offers more power for recommender system.

========================================

The challenges of food waste and insecurity arise in wealthy and developing nations alike, impacting millions of livelihoods. The ongoing pandemic only exacerbates the problem. A major force to combat food waste and insecurity, food rescue (FR) organizations match food donations to the non-profits that serve low-resource communities. Since they rely on external volunteers to pick up and deliver the food, some FRs use web-based mobile applications to reach the right set of volunteers. In this paper, we propose the first machine learning based model to improve volunteer engagement in the food waste and security domain. We (1) develop a recommender system to send push notifications to the most likely volunteers for each given rescue, (2) leverage a mathematical programming based approach to diversify our recommendations, and (3) propose an online algorithm to dynamically select the volunteers to notify without the knowledge of future rescues. Our recommendation system improves the hit ratio from 44% achieved by the previous method to 73%. A pilot study of our method is scheduled to take place in the near future.

========================================

Recommendation systems are information filtering mechanisms used in E-commerce, media and entertainment industry. It essentially facilitate the customers for a better user experience by processing the content user-specific. This is known as personalization. However, though leveraged by machine learning algorithms existing recommendation systems, still suffers from the problem of cold-start and sparcity. These problems could be resolved by using knowledge graphs since it gives a semantic explanation of recommendations. Also, graph learning method overcomes the problems of manual feature extraction and is effective for feature learning in predicting tasks. In this research, we develop a semantic based recommender through link prediction in a knowledge graph. We apply graph embedding techniques for extracting the semantics of explicable recommendations. The proposed method is validated by building a knowledge graph using the MovieLens dataset. We observed that factorization based scoring functions such as HolE and DistMult provides better semantic recommendations.

========================================

Due to privacy and security constraints, directly sharing user data between parties is undesired. Such decentralized data silo issues commonly exist in recommender systems. In general, recommender systems are data-driven. The more data it uses, the better performance it obtains. The data silo issues is a severe limitation of the recommender’s performance. Federated learning is an emerging technology, which bridges the data silos and builds machine learning models without compromising user privacy and data security. We design a recommender system based on federated learning. It is known as the federated recommender system. The system implements plenty of popular algorithms to support various online recommendation services. The algorithm implementation is open-sourced. We also deploy the system on a real-world content recommendation application, achieving significant performance improvement. In this demonstration, we present the architecture of the federated recommender system and give an online demo to show its detailed working procedures and results in content recommendations.

========================================

According to the user profile, a recommender system intends to offer items to the user that may interest him. The recommendations have been applied successfully in various fields. Recommended items include movies, books, travel and tourism services, friends, research articles, research queries, and much more. Hence the presence of recommender systems in many areas, in particular, movies recommendations. Most current Machine Learning recommender systems serve as black boxes that do not provide the user with any insight into or justification for the system's logic. What puts users at risk of losing their confidence. Recommender systems suffer from an overload of information, which poses numerous problems, including high cost, slow data processing, and low time complexity. That is why researchers in have been using graph embeddings algorithms in the recommendation field to reduce the quantity of data, as these algorithms have been successful in the last few years. This work aims to improve the quality of recommendation and the simplicity of recommendation explanation based on the word2vec graph embeddings model. Keywords—Recommender system; explainable artificial intelligence machine learning; Word2vec

========================================

Currently, the user profile based online recommender system has become a hit both in research and engineering domain. Accurately capturing users' profile is the key of recommendation. Recently, lots of researches on user profile extraction have been launched, including content-based recommendation. To better capture users' profiles, a three-step profiling method is adopted in this work. (1) Purchase item prediction is made based on Logistic Regression. (2) Purchase category prediction is made based on support vector machine (SVM), and (3) User's rating prediction is made based on convolutional neural network (CNN) and Long Short-Term Memory (LSTM). This work outperformed the baseline model on the user dataset collected from Amazon. So, in conclusion, the work has the ability of giving reasonable recommendation for users who would like to purchase online. In the future, the video signal processing techniques will also be taken under consideration to capture users' face expression for better recommendation.

========================================

Recommender systems attempt to identify and recommend the most preferable item (product-service) to individual users. These systems predict user interest in items based on related items, users, and the interactions between items and users. We aim to build an auto-routine and color scheme recommender system for home-based smart lighting that leverages a wealth of historical data and machine learning methods. We utilize an unsupervised method to recommend a routine for smart lighting. Moreover, by analyzing users’ daily logs, geographical location, temporal and usage information, we understand user preferences and predict their preferred light colors. To do so, users are clustered based on their geographical information and usage distribution. We then build and train a predictive model within each cluster and aggregate the results. Results indicate that models based on similar users increases the prediction accuracy, with and without prior knowledge about user preferences.

========================================


 
 Learning materials are increasingly available on the Web making them an excellent source of information for building e-Learning recommendation systems. However, learners often have difficulty finding the right materials to support their learning goals because they lack sufficient domain knowledge to craft effective queries that convey what they wish to learn. The unfamiliar vocabulary often used by domain experts creates a semantic gap between learners and experts, and also makes it difficult to map a learner's query to relevant learning materials. We build an e-Learning recommender system that uses background knowledge extracted from a collection of teaching materials and encyclopedia sources to support the refinement of learners' queries. Our approach allows us to bridge the gap between learners and teaching experts. We evaluate our method using a collection of realistic learner queries and a dataset of Machine Learning and Data Mining documents. Evaluation results show our method to outperform benchmark approaches and demonstrates its effectiveness in assisting learners to find the right materials.
 


========================================

In the field of Artificial Intelligence Machine learning provides the automatic systems which learn and improve itself from experience without being explicitly programmed. In this research work a movie recommender system is built using the K-Means Clustering and K-Nearest Neighbor algorithms. The movielens dataset is taken from kaggle. The system is implemented in python programming language. The proposed work deals with the introduction of various concepts related to machine learning and recommendation system. In this work, various tools and techniques have been used to build recommender systems. Various algorithms such as K-Means Clustering, KNN, Collaborative Filtering, Content-Based Filtering have been described in detail. Further, after studying different types of machine learning algorithms, there is a clear picture of where to apply which algorithm in different areas of industries such as recommender systems, e-commerce, etc. Then there is an illustration of how implementations and working of the proposed system are used for the implementation of the movie recommender system. Various building blocks of the proposed system such as Architecture, Process Flow, Pseudo Code, Implementation and Working of the System is described in detail. Finally, in this work for different cluster values, different values of Root Mean Squared Error are obtained. In this proposed work as the no of clusters decreases, the value of RMSE also decreases. The best value of RMSE obtained is 1.081648. The results given by the proposed system are better than the existing technique on the basis of RMSE value.

========================================

Realistic recommender systems are often required to adapt to ever-changing data and tasks or to explore different models systematically. To address the need, we present AutoRec 1 2, an open-source automated machine learning (AutoML) platform extended from the TensorFlow [3] ecosystem and, to our knowledge, the first framework to leverage AutoML for model search and hyperparameter tuning in deep recommendation models. AutoRec also supports a highly flexible pipeline that accommodates both sparse and dense inputs, rating prediction and click-through rate (CTR) prediction tasks, and an array of recommendation models. Lastly, AutoRec provides a simple, user-friendly API. Experiments conducted on the benchmark datasets reveal AutoRec is reliable and can identify models which resemble the best model without prior knowledge.

========================================

A recommendation system can assist the user to compose an understanding of requirements and propose informed decisions from a lot of complicated knowledge. Recommendation from an analysis of sentiments seems to be a great challenge as user-generated content is represented using human language in several complicated ways. Many studies have focused on common fields such as reviews of electrical items, films, and restaurants, but not enough on health and medical issues. Sentiment analysis of healthcare in general and that of the drug experiences of individuals, in particular, may shed considerable light on how to focus on improving public health and reach the correct decision. In this paper, we design and implement a drug recommender system framework that applies sentiment analysis technologies on drug reviews. The objective of this research is to build a decision-making support platform to help patients to achieve more significant choices in drug selection. Firstly, we propose a sentimental measurement approach to drug reviews and generate ratings on drugs. Secondly, we take how much the drug reviews are useful to users, patient's conditions, and dictionary sentiment polarity of drug reviews into consideration. Then, we fuse those factors into the recommendation system to list appropriate medications. Experiments have been carried out using Decision Tree, K-Nearest Neighbors, and Linear Support Vector Classifier algorithm in rating generation and Hybrid model in recommendation based on the given open dataset. The analysis is carried out to tune the parameters for each algorithm in order to achieve greater performance. Finally, Linear Support Vector Classifier is selected for rating generation to obtain a good trade-off among model accuracy, model efficiency, and model scalability.

========================================

Recommender systems (RSs) have been used to successfully address the information overload problem by providing personalized and targeted recommendations to the end users. RSs are software tools and techniques providing suggestions for items to be of use to a user, hence, they typically apply techniques and methodologies from Data Mining. The main contribution of this paper is to introduce a new user profile learning model to promote the recommendation accuracy of vertical recommendation systems. The proposed profile learning model employs the vertical classifier that has been used in multi classification module of the Intelligent. Adaptive Vertical Recommendation (IAVR) system to discover the user’s area of interest, and then build the user’s profile accordingly. Experimental results have proven the effectiveness of the proposed profile learning model, which accordingly will promote the recommendation accuracy

========================================

Abstract Objective To assess usability and usefulness of a machine learning-based order recommender system applied to simulated clinical cases. Materials and Methods 43 physicians entered orders for 5 simulated clinical cases using a clinical order entry interface with or without access to a previously developed automated order recommender system. Cases were randomly allocated to the recommender system in a 3:2 ratio. A panel of clinicians scored whether the orders placed were clinically appropriate. Our primary outcome included the difference in clinical appropriateness scores. Secondary outcomes included total number of orders, case time, and survey responses. Results Clinical appropriateness scores per order were comparable for cases randomized to the order recommender system (mean difference -0.11 order per score, 95% CI: [-0.41, 0.20]). Physicians using the recommender placed more orders (median 16 vs 15 orders, incidence rate ratio 1.09, 95%CI: [1.01-1.17]). Case times were comparable with the recommender system. Order suggestions generated from the recommender system were more likely to match physician needs than standard manual search options. Physicians used recommender suggestions in 98% of available cases. Approximately 95% of participants agreed the system would be useful for their workflows. Discussion User testing with a simulated electronic medical record interface can assess the value of machine learning and clinical decision support tools for clinician usability and acceptance before live deployments. Conclusions Clinicians can use and accept machine learned clinical order recommendations integrated into an electronic order entry interface in a simulated setting. The clinical appropriateness of orders entered was comparable even when supported by automated recommendations.

========================================

Recommender Systems have become a very useful tool for a large variety of domains. Researchers have been attempting to improve their algorithms in order to issue better predictions to the users. However, one of the current challenges in the area refers to how to properly evaluate the predictions generated by a recommender system. In the extent of offline evaluations, some traditional concepts of evaluation have been explored, such as accuracy, Root Mean Square Error and P@N for top-k recommendations. In recent years, more research have proposed some new concepts such as novelty, diversity and serendipity. These concepts have been addressed with the goal to satisfy the users’ requirements. Numerous definitions and metrics have been proposed in previous work. On the absence of a specific summarization on evaluations of recommendation combining traditional metrics and recent progresses, this paper surveys and organizes the main research that present definitions about concepts and propose metrics or strategies to evaluate recommendations. In addition, this survey also settles the relationship between the concepts, categorizes them according to their objectives and suggests potential future topics on user satisfaction.

========================================

Recommender systems are a subclass of information filtering systems. These systems are specialized software components, which usually make part of a larger software system, but can also be standalone tools. A recommender system’s main goal is to provide the user software suggestions for items that can be useful. The suggestions are related to different decision-making mechanisms, different techniques, such as, what product to buy, what movie to watch, or what vacation to reserve. In the context of recommender systems, the general term “item” refers to what the system is actually recommending to its users. The paper presents the development and the comparison of multiple recommendation systems, capable of making item suggestions, based on user, item and user-item interaction data, using different machine learning algorithms. Also, the paper deals with finding different ways of using machine learning models to create recommendation systems, training, evaluating and comparing the different methods in order to provide a general but accurate solution for ranking prediction.

========================================

Recommender systems are increasingly playing an important role in our life, enabling users to find “what they need” within large data collections and supporting a variety of applications, from e-commerce to e-tourism. In this paper, we present a Big Data architecture supporting typical cultural heritage applications. On the top of querying, browsing, and analyzing cultural contents coming from distributed and heterogeneous repositories, we propose a novel user-centered recommendation strategy for cultural items suggestion. Despite centralizing the processing operations within the cloud, the vision of edge intelligence has been exploited by having a mobile app (Smart Search Museum) to perform semantic searches and machine-learning-based inference so as to be capable of suggesting museums, together with other items of interest, to users when they are visiting a city, exploiting jointly recommendation techniques and edge artificial intelligence facilities. Experimental results on accuracy and user satisfaction show the goodness of the proposed application.

========================================

Public transportation is a vital service provided to enable a community to carry out daily activities. One of the mass transportations used in an area is a bus. Moreover, the smart transportation concept is an integrated application of technology and strategy in the transportation system. Using smart idea is the key to the application of the Internet of Things. The ways to improve the management transportation system become a bottleneck for the traditional data analytics solution, one of the answers used in machine learning. This paper uses the Artificial Neural Network (ANN) and Support Vector Machine (SVM) algorithm for the best prediction of travel time with a lower error rate on a case study of a university shuttle bus. Apart from predicting the travel time, this study also considers the fuel cost and gas emission from transportation. The analysis of the experiment shows that the ANN outperformed the SVM. Furthermore, a recommender system is used to recommend suitable routes for the chosen scenario. The experiments extend the discussion with a range of future directions on the stipulated field of study.

========================================

Traveling for leisure has become an important part of our society. It has proven time and again its benefits for wellbeing and personal growth. There are many types of tourism and one of them is Accessible Tourism (AT), an ongoing endeavor to ensure that everyone, regardless of condition, has the right to benefit from tourism experiences. Recommender systems (RSs) represent a mature technique for generating clear and personalized suggestions. While being widely researched and used by the tourism academic community and the tourism industry in general, Recommender Systems (RSs) can still do much more for Accessible Tourism (AT). This thesis aims to build a recommender system dedicated to recommending accessible tourism destinations and easy the process of e2e trip planning for people with disabilities. With a modular design, use of ontologies, machine learning techniques and a “start small, define expansion, expand” approach, this recommender system, once built, aims to be validated by real users.

========================================

Federated Learning (FL) is recently explored as a machine learning paradigm to communally gain generalizable knowledge from the data available in a collection of edge devices without the requirement to transfer the data. FL gives rise to the opportunity to train models on edge devices while preserving user's privacy as the data never leaves user's premises. In this paper, we introduce a simple yet efficient extension of FL for recommender systems to improve on personalization and discuss closely-related meta-learning algorithms. Compared to state-of-the-art federated recommenders, our proposed algorithm is simpler and more robust in real-life scenarios. Through experiments on benchmark data, we evaluate our algorithm in root mean squared error (RMSE) of user's rating prediction.

========================================

The convergence of artificial intelligence and machine learning with material science holds significant promise to rapidly accelerate development timelines of new high-performance polymeric materials. Within this context, we report an inverse design strategy for polycarbonate and polyester discovery based on a recommendation system that proposes polymerization experiments that are likely to produce materials with targeted properties. Following recommendations of the system driven by the historical ring-opening polymerization results, we carried out experiments targeting specific ranges of monomer conversion and dispersity of the polymers obtained from cyclic lactones and carbonates. The results of the experiments were in close agreement with the recommendation targets with few false negatives or positives obtained for each class.

========================================

The replacement of traditional shopping fashion by the various modes of online shopping in real-time. Because of traditional shopping, most of them are getting into real feel about the product whichever they buy. The product features will be manually realized by the consumers whereas in online shopping all the consumers believe the descriptive summary of the products and the various factors based on the sold historical data. Now a days modern shopping method is moving gradually towards hitting more number of customers. Here recommendation system playing a vital role in suggesting the product by considering the earlier records and increasing the demand. Many of the consumers are attracted by factors like deals on an item, rating, review, and cost of the product. Through these factors, most of the consumers are attracted to taking online shopping instead of traditional shopping methods. For suggesting the products to consumers, many kinds of recommendation algorithms are applied using machine learning and deep learning technology to train the system automatically by observing the customer behavior patterns. But the believing factors of the product will be forged some time; in such cases, consumers are not satisfied with their expectations. The overall survey of this paper will address the research gap and opportunities with the recommendation system.

========================================

Academic advising is limited in its ability to assist students in identifying academic pathways. Selecting a major and a university is a challenging process rife with anxiety. Students at high school are not sure how to match their interests with their working future or major. Therefore, high school students need guidance and support. Moreover, students need to filter, prioritize and efficiently get appropriate information from the web in order to solve the problem of information overload. This paper represents an approach for developing ontology-based recommender system improved with machine learning techniques to orient students in higher education. The proposed recommender system is an assessment tool for students' vocational strengths and weaknesses, interests and capabilities. The main objective of our ontology-based recommender system is to identify the student requirements, interests, preferences and capabilities to recommend the appropriate major and university for each one.

========================================

This article aims to proposed framework an Intelligent Recommender System (IRS) for students in higher education institutions. This conceptual framework includes problems in predicting student performance, the possibility of graduating on time, and recommends choosing subjects according to performance, and career interests, which are useful for assisting pedagogical interventions in future student development. The success in the development and implementation of the proposed IRS framework is inseparable from using data mining and machine learning techniques in predicting and providing recommendations. Data analysis consisted of clustering techniques, association rules, and classification using Support Vector Machine (SVM), Naïve Bayes, and k-Nearest Neighbour (k-NN). These techniques are used to solve problems related to students and to provide appropriate recommendations. The result is an IRS conceptual framework for the college student that can be used as smart agents to provide student guidance and suggestions to support the process of education in higher education.

========================================

Music, as all other art forms, has been used primarily as a vehicle for conveying ideas, experiences and emotions in a stylistic manner. It thus makes sense to attempt to categorize a library of music into either its style or the emotions expressed in the tracks. In this work, preliminary results of the signal processing module and machine learning module with four songs in detail and with a database of 100 songs is carried out. The signal processing algorithms employed are Mel Frequency Cepstral Coefficients and beat Histogram. Human emotions were classified based on Thayers model into Happy, Sad, Angry and Relaxed. The Machine Learning classification algorithms employed are Decision Tree Classifier and Random Forest Classifier. A low accuracy suggests improvement in the features and better machine learning algorithm before porting to Android for development as a Mobile App.

========================================

Owning to electricity market deregulation, residential customers now enjoy the freedom to choose their preferred electricity retailers. This paper investigates the application of recommender system, a fast-developing technique in machine learning, into the task of recommending electricity plans for the individual residential customer. Based on a collaborative filtering strategy, an electricity plan recommender system (EPRS) is developed. By providing easily obtainable data of some household appliances, residential customers of the EPRS are recommended with predicted ratings of different plans, which can provide effective guidance to customers in the selection of suitable plans and proper tariffs. Different numerical tests are carried out to evaluate the performance of the EPRS. The EPRS outperforms other strategies in the accuracy of recommendation result and is verified to be a promising solution to electricity plan recommendation task.

========================================

E-learning platforms contain a large number of heterogeneous resources of knowledge. In current e-learning systems, learners spend a lot of time and effort trying to find relevant learning resources. It is necessary to consider the learner's true needs according to different factors, such as learning style, experience, and preferences. Learning needs to be relevant to the context of the required concept. This work presents analytical review of the current status of e-learning system design. A new approach based on machine learning (ML) combined with ontology techniques has been suggested to develop an efficient e-learning recommender system. The proposed model uses Bayesian inference algorithm to predict learning materials, which are collected and indexed within the system. Ontology has been used to expand the initial terms extracted. Semantic relation between the learning material and the term was generated and fed to the model. Experimental results using this model show promising performance.

========================================

We present the Automated List Inspection (ALI) tool that utilizes methods from machine learning, natural language processing, combined with domain expert knowledge to automate financial statement auditing. ALI is a content based context-aware recommender system, that matches relevant text passages from the notes to the financial statement to specific law regulations. In this paper, we present the architecture of the recommender tool which includes text mining, language modeling, unsupervised and supervised methods that range from binary classification models to deep recurrent neural networks. Next to our main findings, we present quantitative and qualitative comparisons of the algorithms as well as concepts for how to further extend the functionality of the tool.

========================================

ABSTRACT Recommender systems use machine-learning techniques to make predictions about resources. The medical field is one where much research is currently being conducted on recommender system utility. In the last few years, the amount of information available online that relates to healthcare has increased tremendously. Patients nowadays are more aware and look for answers to healthcare problems online. This has resulted in a dire need of an effective reliable online system to recommend the physician that is best suited to a particular patient in a limited time. In this article, a hybrid doctor-recommender system is proposed, by combining different recommendation approaches: content base, collaborative and demographic filtering to effectively tackle the issue of doctor recommendation. The proposed system addresses the issue of personalization through analysing patient's interest towards selecting a doctor. It uses a novel adoptive algorithm to construct a doctor's ranking function. Moreover, this ranking function is used to translate patients’ criteria for selecting a doctor into a numerical base rating, which will eventually be used in the recommendation of doctors. The system has been evaluated thoroughly, and result show that recommendations are reasonable and can fulfil patient's demand for reliable doctor's selection effectively.

========================================

Recommender systems are now a popular research area and have become powerful tools to present personalized offers to users in many domains (e.g. e-commerce, e-learning). In this paper, we introduced an approach of personalization which extracts learners’ relationship based on learning processes and learning activities (e.g. note taking) to provide more authenticity, personalized recommendations for group learning support. Base on learners’ learning activities some interaction factors are extracted by using natural language process technologies and data mining automatically. Then, extracted interaction factors are utilized to generate some relationship indicators for inferring the learners’ directive relationship. These indicators are as symbols in order to describe a situation and relative degree which knowledge and understanding are socially distributed among group learners. Thirdly, we use a machine learning approach for acquiring a learner relationship identify module according to the relationship indicators. The experimental result shows that the proposed approach can give a more satisfying and qualified recommendation.

========================================

The main focus of the paper is to propose a smart recommender system based on the methods of hybrid learning for personal well-being services, called a smart recommender system of hybrid learning (SRHL). The essential health factor is considered to be personal lifestyle, with the help of a critical examination of various disciplines. Integrating the recommender system effectively contributes to the prevention of disease, and it also leads to a reduction in treatment cost, which contributes to an improvement in the quality of life. At the same time, there exist various challenges within the recommender system, mainly cold start and scalability. To effectively address the inefficiencies, we propose combined hybrid methods in regard to machine learning. The primary aim of this learning method is to integrate the most effective and efficient learning algorithms to examine how combined hybrid filtering can help to improve the cold star problem efficiently in the provision of personalized well-being in regard to health food service. These methods include: (1) switching among content-based and collaborative filtering; (2) identifying the user context with the integration of dynamic filtering; and (3) learning the profiles with the help of processing and screening of reflecting feedback loops. The experimental results were evaluated by using three absolute error measures, providing comparable results with other studies relative to machine learning domains. The effects of using the hybrid learning method are gathered with the help of the experimental results. Our experiments also show that the hybrid method improves accuracy by 14.61% of the average error predicted in the recommender systems in comparison to the collaborative methods, which mainly focus on the computation of similar entities.

========================================

Coordination failure reduces match quality among employers and candidates in the job market, resulting in a large number of unfilled positions and/or unstable, short-term employment. Centralized job search engines provide a platform that connects directly employers with job-seekers. However, they require users to disclose a significant amount of personal data, i.e., build a user profile, in order to provide meaningful recommendations. In this paper, we present PrivateJobMatch - a privacy-oriented deferred multi-match recommender system - which generates stable pairings while requiring users to provide only a partial ranking of their preferences. PrivateJobMatch explores a series of adaptations of the game-theoretic Gale-Shapley deferred acceptance algorithm which combine the flexibility of decentralized markets with the intelligence of centralized matching. We identify the shortcomings of the original algorithm when applied to a job market and propose novel solutions that rely on machine learning techniques. Experimental results on real and synthetic data confirm the benefits of the proposed algorithms across several quality measures. Over the past year, we have implemented a PrivateJobMatch prototype and deployed it in an active job market economy. Using the gathered real-user preference data, we find that the match recommendations are superior to a typical decentralized job market---while requiring only a partial ranking of the user preferences.

========================================

With ties among people have been much more closer, making recommendations for groups of users became a more general demand, which facilitates the prevalence of group recommender system (GRS). Existing solutions for GRS are mostly established based on preference feedbacks of absolute form such as ratings, yet neglecting that preference assessment criteria are usually heterogeneous among different members. In this paper, we propose GRS-PR, an enhanced group recommender system by exploiting preference relation. First, a preference relation-based multi-variate extreme learning machine model is formulated to predict unknown preference relations in candidate items. Second, on the basis of predicted results, borda voting rule is employed to generate recommendation results from candidate items. In addition, efficiency, parameter sensitivity, and sparsity tolerance of the GRS-PR are evaluated through a set of experiments.

========================================

We propose a machine-learning method to recommend successful processing conditions for new compounds on the basis of parallel experiments. Initially, an experimental database was constructed for 67...

========================================

In this talk, we will go over the components of personalized search and recommender systems and demonstrate the applications of various deep learning techniques along the way. Search and recommender systems are probably the most prevalent ML powered application across the industry. They share most of the components composition and provide a user a ranked list of items, while there is subtle difference that a search system typically acts passively with a clear user intention in terms of queries and a recommender system acts more proactively. Deep learning has been wildly successful in solving complex tasks such as image recognition, speech recognition, natural language processing and understanding, machine translation, etc. In the area of personalized recommender systems, deep learning has been showing tremendous impact in recent years. Search and recommender systems can be staged roughly in three phases: 1. User and query understanding, where a query or a user profile are processed so that the systems can use the processed information to 2. retrieve all the related items (high recall) and 3. rank the items by the order of the most relevance to the user's intent (high precision). Each phase has its unique challenges but deep learning has been ubiquitously pushing beyond the limit. After walking through the talk, we hope the audience would gain some first-hand experience building a personalized search/recommender system using deep learning techniques.

========================================

This paper focuses on building personalized recommender system in the tourism field. The application recommends to a tourist the best attractions in a particular place according to his preferences, his profile and his appreciation to previous visited places. This paper proposes a hybrid recommender system that combines the three most known recommender methods which are: the collaborative filtering (CF), the content-based filtering (CB) and the demographic filtering (DF). In order to implement these recommender methods, we have applied different machine learning algorithms which are the K-nearest neighbors (K-NN) for both CB and CF and the decision tree for the DF. The hybridization is a good choice to make the best of their advantages and to overcome the cold start problem. To enhance the recommendation accuracy, we use two hybridization techniques: switching and weighted. For the weighted approach, a novel linear programming model is applied to obtain the optimal weights' values. An extensive experimental study is conducted based on different evaluation metrics using extracted data from TripAdvisor. Our results show that the hybrid method is more accurate than the other recommender approaches used separately.

========================================

This paper1 presents a smart freight transporting system that uses machine learning in order to optimize resources and a mobile apps to display results to potential users. In the proposed framework, we developed a clustering based recommender system for customers, strategic objectives are statements that indicate what is critical to the final decision, and the proposed smart system has as an objective to reduce CO2 emissions, fuel consumption, and congestion by reducing the waiting time of the customers. The framework has the potential to identify more efficient and affordable solutions for traffic management and customer satisfactions.

========================================

Singular value decomposition (SVD) is the mathematical basis of principal component analysis (PCA). Together, SVD and PCA are one of the most widely used mathematical formalism/decomposition in machine learning, data mining, pattern recognition, artificial intelligence, computer vision, signal processing, etc. In recent applications, regularization becomes an increasing trend. In this paper, we present a regularized SVD (RSVD), present an efficient computational algorithm, and provide several theoretical analysis. We show that although RSVD is non-convex, it has a closed-form global optimal solution. Finally, we apply RSVD to the application of recommender system and experimental result show that RSVD outperforms SVD significantly.

========================================

With the advent of web based e-learning systems, a huge amount of educational data is getting generated. These massive data gave rise to Big data in educational sectors. Currently, big data analytics techniques are being used to analyze these educational data and generate different predictions and recommendations for students, teachers and schools. Recommendation systems are already very helpful in e-commerce, service industry and social networking sites. Recently recommendation systems are proved to be efficient for education sector as well. In this work we are using recommendation system for Big data in education. This work uses collaborative filtering based recommendation techniques to recommend elective courses to students, depending upon their grade points obtained in other subjects. We are using item based recommendation of Mahout machine learning library on top of Hadoop to generate set of recommendations. Similarity Log-likelihood is used to discover patterns among grades and subjects. Root Mean Square Error between actual grade and recommended grade is used to test the recommendation system. The output of this study can be used by schools, colleges or universities to suggest alternative elective courses to students.

========================================

Structures and properties of many inorganic compounds have been collected historically. However, it only covers a very small portion of possible inorganic crystals, which implies the presence of numerous currently unknown compounds. A powerful machine-learning strategy is mandatory to discover new inorganic compounds from all chemical combinations. Herein we propose a descriptor-based recommender-system approach to estimate the relevance of chemical compositions where crystals can be formed [i.e., chemically relevant compositions (CRCs)]. In addition to data-driven compositional similarity used in the literature, the use of compositional descriptors as a prior knowledge is helpful for the discovery of new compounds. We validate our recommender systems in two ways. First, one database is used to construct a model, while another is used for the validation. Second, we estimate the phase stability for compounds at expected CRCs using density functional theory calculations.

========================================

Spectrum occupancy prediction is a key enabler of agile and proactive decision-making for dynamic spectrum management. In this paper, state-of-the- art statistical models and machine learning prediction methods are evaluated on real-world occupancy time series measured in the Land Mobile Radio bands. While there is no universally best method for forecasting spectrum usage data, significant accuracy improvements are shown to be achievable by selecting a suitable prediction method for different frequencies. Motivated by this observation, we treat the problem of automating the selection of a suitable prediction method from a candidate pool as a machine learning task. An approach is proposed that recommends the best performing method for new data instances by learning from prior predictions on spectrum data with similar characteristics. The merit of this approach is shown in terms of improving the prediction accuracy compared to baseline selections without the recommender.

========================================

Different efforts have been made to address the problem of information overload on the Internet. Recommender systems aim at directing users through this information space, toward the resources that best meet their needs and interests. Web Content Recommendation has been an active application area for Information Filtering, Web Mining and Machine Learning research. Recent studies show that combining the conceptual and usage information can improve the quality of web recommendations. In this paper we exploit this idea to enhance a reinforcement learning framework, primarily devised for web recommendations based on web usage data. A hybrid web recommendation method is proposed by making use of the conceptual relationships among web resources to derive a novel model of the problem, enriched with semantic knowledge about the usage behavior. With our hybrid model for the web page recommendation problem we show the apt and flexibility of the reinforcement learning framework in the web recommendation domain, and demonstrate how it can be extended in order to incorporate various sources of information. We evaluate our method under different settings and show how this method can improve the overall quality of web recommendations.

========================================

In order to enable students to directly face empirical data, summarize translation rules and learn translation skills, this paper studies the basis, motivation and methods of applying research dynamics in translation and teaching. Presenting data in class is the main method of dynamically searching corpora, which enables learners to face enough bilingual data that are easy to choose, and makes translation skills and teaching of translation of selected language items relatively focused. In recent years, the emotional analysis text has attracted academic scientists, and the professionals involved in the research, the use of research methods, and the cultural background related to language have become more and more extensive. In this paper, natural language processing is used to analyze emotions contained in translated texts. Natural language processing not only helps to manage the huge ability of data to efficiently translate text, but also helps to extract the hidden emotions in text translation. It only takes half the effort to achieve the multiplier effect. The multi label classification in natural language processing can reflect the information contained in emotion. The translated text is more detailed, which is helpful for further research.

========================================

Background In recent years, the intersection of natural language processing (NLP) and public health has opened innovative pathways for investigating social determinants of health (SDOH) in textual datasets. Despite the promise of NLP in the SDOH domain, the literature is dispersed across various disciplines, and there is a need to consolidate existing knowledge, identify knowledge gaps in the literature, and inform future research directions in this emerging field. Objective This research protocol describes a systematic review to identify and highlight NLP techniques, including large language models, used for SDOH-related studies. Methods A search strategy will be executed across PubMed, Web of Science, IEEE Xplore, Scopus, PsycINFO, HealthSource: Academic Nursing, and ACL Anthology to find studies published in English between 2014 and 2024. Three reviewers (SR, ZZ, and YC) will independently screen the studies to avoid voting bias, and two (AS and YX) additional reviewers will resolve any conflicts during the screening process. We will further screen studies that cited the included studies (forward search). Following the title abstract and full-text screening, the characteristics and main findings of the included studies and resources will be tabulated, visualized, and summarized. Results The search strategy was formulated and run across the 7 databases in August 2024. We expect the results to be submitted for peer review publication in early 2025. As of December 2024, the title and abstract screening was underway. Conclusions This systematic review aims to provide a comprehensive study of existing research on the application of NLP for various SDOH tasks across multiple textual datasets. By rigorously evaluating the methodologies, tools, and outcomes of eligible studies, the review will identify gaps in current knowledge and suggest directions for future research in the form of specific research questions. The findings will be instrumental in developing more effective NLP models for SDOH, ultimately contributing to improved health outcomes and a better understanding of social determinants in diverse populations. International Registered Report Identifier (IRRID) DERR1-10.2196/66094

========================================

. Background: Anti-neutrophil cytoplasmatic antibody (ANCA)-associated vasculitis (AAV) is a rare, life-threatening, systemic auto-immune disease. Due to the low prevalence and heterogenous registration, there is an urgent need to improve identification of AAV patients within the electronic health record (EHR)-system of health organizations to facilitate clinical research. Methods: Our aim was to identify, with a high sensitivity, low-prevalence AAV patients within large EHR-systems (>2.000.000 records) using an artificial intelligence (AI)-search tool. We combined a search on structured and unstructured data with natural language processing (NLP)-based exclusion. We developed the method in an academic center with an established AAV training set (n=203) and validated the method in a non-academic center with a validation set (n=84). We anonymously reviewed all identified patient records for AAV diagnosis. Results: The final search strategy combined four queries on disease description, laboratory measurements, medication and specialisms. In the training center, this search identified 608 patients, of which 346 were AAV patients upon manual review. 197/203 patients of the training set were retrieved, indicating a sensitivity of 97%. Employing NLP-based exclusion resulted in 444 patients with 339 AAV patients, resulting in an increase of positive predictive value (PPV) from 57% to 78% and a sensitivity of 96%. In the validation center the search strategy identified 333 patients, of which 194 were AAV patients, including 82/84 (98%) patients of the validation set. After NLP-based exclusion 223 patients remained, including 196 AAV patients, improving PPV from 58 to 86% with a sensitivity of 98%. Our identification method outperformed ICD-10 coding predominantly in identifying myeloperoxidase (MPO)-positive AAV patients and patients with few specialisms involved. Conclusions: We demonstrated excellent performance of an AI-based identification method, incorporating NLP, to identify AAV patients in EHRs and we validated the applicability and transportability. This method can accelerate research efforts, while avoiding the limitations of ICD-10-based registration.

========================================

The study focuses on the use of Natural Language Processing (NLP) to enhance the efficiency of digital services. The primary goal is to evaluate and analyze research published over a five-year span regarding NLP and its impact on digital services. This includes the predominant publishing mediums, high-impact journals, recurring keywords, and the geographical contribution in this domain. The research was conducted following the guidelines set by Kitchenham and Charters. The search was centered on well-established academic databases, using advanced search queries with pertinent keywords. Selected articles were assessed based on specific criteria, and key information from each article was extracted using the Mendeley Desktop tool. Research outcomes highlighted an upward trend in publishing NLP studies in high-impact journals, with a notable contribution from the United States, China, India, and the United Kingdom. The recurring keywords underscored the link between NLP, Machine Learning, and Deep Learning. Additionally, the rising significance of NLP in improving digital services was observed, indicating a bright future for its role in digital transformation.

========================================

Background Health researchers are increasingly using natural language processing (NLP) to study various mental health conditions using both social media and electronic health records (EHRs). There is currently no published synthesis that relates specifically to the use of NLP methods for bipolar disorder, and this scoping review was conducted to synthesize valuable insights that have been presented in the literature. Objective This scoping review explored how NLP methods have been used in research to better understand bipolar disorder and identify opportunities for further use of these methods. Methods A systematic, computerized search of index and free-text terms related to bipolar disorder and NLP was conducted using 5 databases and 1 anthology: MEDLINE, PsycINFO, Academic Search Ultimate, Scopus, Web of Science Core Collection, and the ACL Anthology. Results Of 507 identified studies, a total of 35 (6.9%) studies met the inclusion criteria. A narrative synthesis was used to describe the data, and the studies were grouped into four objectives: prediction and classification (n=25), characterization of the language of bipolar disorder (n=13), use of EHRs to measure health outcomes (n=3), and use of EHRs for phenotyping (n=2). Ethical considerations were reported in 60% (21/35) of the studies. Conclusions The current literature demonstrates how language analysis can be used to assist in and improve the provision of care for people living with bipolar disorder. Individuals with bipolar disorder and the medical community could benefit from research that uses NLP to investigate risk-taking, web-based services, social and occupational functioning, and the representation of gender in bipolar disorder populations on the web. Future research that implements NLP methods to study bipolar disorder should be governed by ethical principles, and any decisions regarding the collection and sharing of data sets should ultimately be made on a case-by-case basis, considering the risk to the data participants and whether their privacy can be ensured.

========================================

Background The COVID-19 pandemic has highlighted the growing need for digital learning tools in postgraduate family medicine training. Family medicine departments must understand and recognize the use and effectiveness of digital tools in order to integrate them into curricula and develop effective learning tools that fill gaps and meet the learning needs of trainees. Objective This scoping review will aim to explore and organize the breadth of knowledge regarding digital learning tools in family medicine training. Methods This scoping review follows the 6 stages of the methodological framework outlined first by Arksey and O’Malley, then refined by Levac et al, including a search of published academic literature in 6 databases (MEDLINE, ERIC, Education Source, Embase, Scopus, and Web of Science) and gray literature. Following title and abstract and full text screening, characteristics and main findings of the included studies and resources will be tabulated and summarized. Thematic analysis and natural language processing (NLP) will be conducted in parallel using a 9-step approach to identify common themes and synthesize the literature. Additionally, NLP will be employed for bibliometric and scientometric analysis of the identified literature. Results The search strategy has been developed and launched. As of October 2021, we have completed stages 1, 2, and 3 of the scoping review. We identified 132 studies for inclusion through the academic literature search and 127 relevant studies in the gray literature search. Further refinement of the eligibility criteria and data extraction has been ongoing since September 2021. Conclusions In this scoping review, we will identify and consolidate information and evidence related to the use and effectiveness of existing digital learning tools in postgraduate family medicine training. Our findings will improve the understanding of the current landscape of digital learning tools, which will be of great value to educators and trainees interested in using existing tools, innovators looking to design digital learning tools that meet current needs, and researchers involved in the study of digital tools. Trial Registration OSF Registries osf.io/wju4k; https://osf.io/wju4k International Registered Report Identifier (IRRID) DERR1-10.2196/34575

========================================


 BACKGROUND
 Introduction
The COVID-19 pandemic has highlighted the growing need for digital learning tools in postgraduate family medicine training. Family medicine departments must understand and recognize the use and effectiveness of digital tools in order to integrate them into curricula and develop effective learning tools that fill gaps and meet the learning needs of trainees.
 
 
 OBJECTIVE
 This scoping review will aim to explore and organize the breadth of knowledge regarding digital learning tools in family medicine training.
 
 
 METHODS
 This scoping review will follow the methodological framework outlined by Arksey and O’Malley, including a search of published academic literature in six databases (MEDLINE, ERIC, Education Source, Embase, Scopus, and Web of Science) and grey literature. Following title/abstract, and full text screening, characteristics and main findings of the included studies and resources will be tabulated and summarized. Thematic analysis and natural language processing will be conducted to identify common themes and synthesize the literature. Additionally, natural language processing (NLP) will be employed for bibliometric and scientometric analysis of the identified literature.
 
 
 RESULTS
 The search strategy has been developed and launched. Data extraction is currently underway.
 
 
 CONCLUSIONS
 In this scoping review, we will identify and consolidate information and evidence related to the use and effectiveness of existing digital learning tools in postgraduate family medicine training. Our findings will improve the understanding of the current landscape of digital learning tools, which will be of great value to educators and trainees interested in using existing tools, to innovators looking to design digital learning tools that meet current needs, and to researchers involved in the study of digital tools.


========================================

SUMMARY
We present a web server, GenCLiP 3, which is an updated version of GenCLiP 2.0 to enhance analysis of human gene functions and regulatory networks, with the following improvements: i) accurate recognition of molecular interactions with polarity and directionality from the entire PubMed database; ii) support for Boolean search to customize multiple-term search and to quickly retrieve function related genes; iii) strengthened association between gene and keyword by a new scoring method; and iv) daily updates following literature release at PubMed FTP.


AVAILABILITY
The server is freely available for academic use at: http://ci.smu.edu.cn/genclip3/.


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.

========================================

Background: Prior studies using structured EHR data suggest that patients with prediabetes are not receiving evidence-based care, but prediabetes may be discussed in unstructured data. We developed and validated an NLP tool to identify discussions about prediabetes in EHR notes. Methods: We included adult patients without diabetes with an in-person office visit at any general medicine clinic at a single academic center and at least one HbA1c 5.7-6.4% between 7/1/2016 and 12/31/2018. In phase 1, we devised an initial keyword search strategy based on clinical experience. We extracted notes with at least one of the keywords and manually annotated them to determine whether they represented clinical discussions of prediabetes. We used the annotated notes to train and evaluate multiple machine learning and deep learning classifiers to replicate human annotation. In phase 2, we applied a similar annotation process and machine learning method on notes from a different group of clinic practices. We analyzed notes from phase 2 to describe the content of prediabetes discussions: labs ordered or reviewed (HbA1c or fasting glucose); lifestyle counseling; diabetes prevention program (DPP) discussion/referral; nutrition referral/discussion; and metformin discussion or ordering/continuation. Results: We identified 269 patients with prediabetes discussions in phase 2. Most commonly, PCPs provided lifestyle counseling (80%), reviewed current labs (63%) and ordered follow-up labs (60%). PCPs rarely discussed/referred to a nutritionist (4%). We did not find any discussions/referrals to a DPP. Metformin was discussed, ordered or continued in Conclusions: We developed and validated a NLP tool that identifies clinical discussions about prediabetes in the EHR. PCPs most commonly provided lifestyle counseling in the office and infrequently placed referrals to nutrition or DPPs, which have strong evidence for preventing progression to diabetes. Disclosure E. Tseng: None. J.L. Schwartz: None. M. Rouhizadeh: None. N.M. Maruthur: Other Relationship; Self; Johns Hopkins HealthCare Solutions. Funding National Institute of Diabetes and Digestive and Kidney Diseases (1K23DK118205-01A1)

========================================

OBJECTIVE
Delayed diagnosis of Kawasaki disease (KD) may lead to serious cardiac complications. We sought to create and test the performance of a natural language processing (NLP) tool, the KD-NLP, in the identification of emergency department (ED) patients for whom the diagnosis of KD should be considered.


METHODS
We developed an NLP tool that recognizes the KD diagnostic criteria based on standard clinical terms and medical word usage using 22 pediatric ED notes augmented by Unified Medical Language System vocabulary. With high suspicion for KD defined as fever and three or more KD clinical signs, KD-NLP was applied to 253 ED notes from children ultimately diagnosed with either KD or another febrile illness. We evaluated KD-NLP performance against ED notes manually reviewed by clinicians and compared the results to a simple keyword search.


RESULTS
KD-NLP identified high-suspicion patients with a sensitivity of 93.6% and specificity of 77.5% compared to notes manually reviewed by clinicians. The tool outperformed a simple keyword search (sensitivity = 41.0%; specificity = 76.3%).


CONCLUSIONS
KD-NLP showed comparable performance to clinician manual chart review for identification of pediatric ED patients with a high suspicion for KD. This tool could be incorporated into the ED electronic health record system to alert providers to consider the diagnosis of KD. KD-NLP could serve as a model for decision support for other conditions in the ED.

========================================

DOI: 10.14254/2071789X.2018/11-4/11 ABSTRACT. This study continues a series of studies on the effectiveness of scientific conferences. This topic has not been sufficiently investigated although it receives large funds, assuming that these conferences have added value for staff members' academic-professional development. Predicated on questionnaires filled by 96 academic staff members from 17 different departments, we found that when choosing conferences to attend, the type of faculty affect the search for cooperation. Moreover, staff members from the Faculty of Natural Sciences attribute more significance to conferences that result in publications than staff members from the Faculty of Health. The Faculty of Engineering creates negative mediation in the correlation between gender and cooperation. Namely, the Faculty of Engineering does not urge cooperation and even has a negative effect, but its effect is evident mainly among men. This finding complements prior research findings showing that women are more inclined to cooperation (Eckhaus & Davidovitch, 2018a). The current findings show that the inclination to cooperation is not related only to gender issues rather the faculty has an effect as well. The current findings might have a contribution to the significance of the faculty as an influential factor of conferences on cooperation – and in fact on the professional development of staff members.

========================================

We introduce Talk to Papers, which exploits the recent open-domain question answering (QA) techniques to improve the current experience of academic search. It’s designed to enable researchers to use natural language queries to find precise answers and extract insights from a massive amount of academic papers. We present a large improvement over classic search engine baseline on several standard QA datasets and provide the community a collaborative data collection tool to curate the first natural language processing research QA dataset via a community effort.

========================================

Academic paper search is an essential task for efficient literature discovery and scientific advancement. While dense retrieval has advanced various ad-hoc searches, it often struggles to match the underlying academic concepts between queries and documents, which is critical for paper search. To enable effective academic concept matching for paper search, we propose Taxonomy-guided Semantic Indexing (TaxoIndex) framework. TaxoIndex extracts key concepts from papers and organizes them as a semantic index guided by an academic taxonomy, and then leverages this index as foundational knowledge to identify academic concepts and link queries and documents. As a plug-and-play framework, TaxoIndex can be flexibly employed to enhance existing dense retrievers. Extensive experiments show that TaxoIndex brings significant improvements, even with highly limited training data, and greatly enhances interpretability.

========================================

As the body of academic literature continues to grow, researchers face increasing difficulties in effectively searching for relevant resources. Existing databases and search engines often fall short of providing a comprehensive and contextually relevant collection of academic literature. To address this issue, we propose a novel framework that leverages Natural Language Processing (NLP) techniques. This framework automates the retrieval, summarization, and clustering of academic literature within a specific research domain. To demonstrate the effectiveness of our approach, we introduce CyLit, an NLP-powered repository specifically designed for the cyber risk literature. CyLit empowers researchers by providing access to context-specific resources and enabling the tracking of trends in the dynamic and rapidly evolving field of cyber risk. Through the automatic processing of large volumes of data, our NLP-powered solution significantly enhances the efficiency and specificity of academic literature searches. We compare the literature categorization results of CyLit to those presented in survey papers or generated by ChatGPT, highlighting the distinctive insights this tool provides into cyber risk research literature. Using NLP techniques, we aim to revolutionize the way researchers discover, analyze, and utilize academic resources, ultimately fostering advancements in various domains of knowledge.

========================================

To effectively assess a scholar's capabilities, it is essential to begin by gathering their basic information, forming the foundational step for subsequent analysis. This entails sourcing pertinent details such as the scholar's affiliation, title, research focus, and published papers from online sources. We curated a comprehensive dataset comprising the homepages of numerous scholars and develop a system to collect information. Our methodology involved harnessing search engines and webpage classifiers to locate scholars' homepages across the internet. Once identified, these pages underwent meticulous parsing to extract academic information. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable proficiency across various domains, particularly in fundamental Natural Language Processing (NLP) tasks. By leveraging LLM capabilities, we conducted text extraction from webpage content, resulting in the successful compilation of academic data.

========================================

The Internet, much like our universe, is ever-expanding. Information, in the most varied formats, is continuously added to the point of information overload. Consequently, the ability to navigate this ocean of data is crucial in our day-to-day lives, with familiar tools such as search engines carving a path through this unknown. In the research world, articles on a myriad of topics with distinct complexity levels are published daily, requiring specialized tools to facilitate the access and assessment of the information within. Recent endeavors in artificial intelligence, and in natural language processing in particular, can be seen as potential solutions for breaking information overload and provide enhanced search mechanisms by means of advanced algorithms. As the advent of transformer-based language models contributed to a more comprehensive analysis of both text-encoded intents and true document semantic meaning, there is simultaneously a need for additional computational resources. Information retrieval methods can act as low-complexity, yet reliable, filters to feed heavier algorithms, thus reducing computational requirements substantially. In this work, a new search engine is proposed, addressing machine reading at scale in the context of scientific and academic research. It combines state-of-the-art algorithms for information retrieval and reading comprehension tasks to extract meaningful answers from a corpus of scientific documents. The solution is then tested on two current and relevant topics, cybersecurity and energy, proving that the system is able to perform under distinct knowledge domains while achieving competent performance.

========================================

Natural language processing (NLP) has been studied in computing for decades. Recent technological advancements have led to the development of sophisticated artificial intelligence (AI) models, such as Chat Generative Pre-trained Transformer (ChatGPT). These models can perform a range of language tasks and generate human-like responses, which offers exciting prospects for academic efficiency. This manuscript aims at (i) exploring the potential benefits and threats of ChatGPT and other NLP technologies in academic writing and research publications; (ii) highlights the ethical considerations involved in using these tools, and (iii) consider the impact they may have on the authenticity and credibility of academic work. This study involved a literature review of relevant scholarly articles published in peer-reviewed journals indexed in Scopus as quartile 1. The search used keywords such as "ChatGPT," "AI-generated text," "academic writing," and "natural language processing." The analysis was carried out using a quasi-qualitative approach, which involved reading and critically evaluating the sources and identifying relevant data to support the research questions. The study found that ChatGPT and other NLP technologies have the potential to enhance academic writing and research efficiency. However, their use also raises concerns about the impact on the authenticity and credibility of academic work. The study highlights the need for comprehensive discussions on the potential use, threats, and limitations of these tools, emphasizing the importance of ethical and academic principles, with human intelligence and critical thinking at the forefront of the research process. This study highlights the need for comprehensive debates and ethical considerations involved in their use. The study also recommends that academics exercise caution when using these tools and ensure transparency in their use, emphasizing the importance of human intelligence and critical thinking in academic work.

========================================

Abstract An ongoing project explores the extent to which artificial intelligence (AI), specifically in the areas of natural language processing and semantic reasoning, can be exploited to facilitate the studies of science by deploying software agents equipped with natural language understanding capabilities to read scholarly publications on the web. The knowledge extracted by these AI agents is organized into a heterogeneous graph, called Microsoft Academic Graph (MAG), where the nodes and the edges represent the entities engaging in scholarly communications and the relationships among them, respectively. The frequently updated data set and a few software tools central to the underlying AI components are distributed under an open data license for research and commercial applications. This paper describes the design, schema, and technical and business motivations behind MAG and elaborates how MAG can be used in analytics, search, and recommendation scenarios. How AI plays an important role in avoiding various biases and human induced errors in other data sets and how the technologies can be further improved in the future are also discussed.

========================================

In today’s digital age, the amount of available research literature is growing exponentially, making it more challenging for researchers to efficiently manage and cite relevant sources. This issue is particularly pronounced in fields like linguistics, where scholars must contend with interdisciplinary sources spanning linguistics, psychology, and computer science. This paper proposes the development of an AI-powered, voice-controlled academic assistant aimed at enhancing the research experience for linguists by streamlining the literature review and citation process. The assistant would use natural language processing (NLP) and machine learning to allow researchers to search for, retrieve, and cite references using voice commands, thereby eliminating many of the tedious aspects of academic research. This proposal outlines the current state of voice-controlled technology, discusses how these technologies can be implemented in academic workflows, and presents a roadmap for the development of a linguist-friendly reference management system. By addressing key technical, ethical, and practical concerns, this paper offers a compelling vision for the future of linguistic research, powered by AI.

========================================

The rapid growth of digital technologies and natural language processing (NLP) have revolutionized the field of education, creating new demand for automated academic assistance systems. In this paper, we present an NLP-based academic assistance chatbot designed to provide comprehensive support to students and researchers using deep learning techniques.  The chatbot incorporates a range of intelligent features to assist with university recommendations, article writing, automatic question answering (QA), and job search. By leveraging sentiment analysis and sarcasm detection models. The proposed chatbot could offer accurate and insightful university recommendations. Additionally, the chatbot incorporates spell and grammar checking, summarization, paraphrasing, and topic modeling capabilities to aid users in enhancing their writing skills. The QA module enables users to obtain quick and precise answers to factoid-based questions. Moreover, the chatbot helps with internships and job search. According to literature, this work presents the first assistance chatbot that encapsulates all features that may be needed by a university student to facilitate and improve his/her learning process. The results demonstrated clearly in the body of the paper showed the success achieved by the academic assistant proposed and built in this work in all its features or modules to offer help to university students and graduates.

========================================

In the recent past, there has been a surge in the use of artificial intelligence (AI) in the development of smart technologies for the purpose of improving efficiency in writing academic papers and conducting researches. However, the potential of using AI in the improvement of scholarly processes has not been optimally realized due to low awareness and visibility of the tool among the users. In this respect, this paper aims to describe the following tools of AI which can be applied in the research process including literature search and manuscript preparation. To assess the AI technology, the current literature in form of case studies was reviewed and this included the automated literature search engines, citation management software, natural language processing tools and data analysis tools. It also reveals that AI approaches can also help in decreasing the amount of time spent in article and data search, citation, citation management, and even in the generation of quality publications. This essay also examines the ethical issues of using artificial intelligence in research and any bias that may be present. In conclusion, it is necessary to underline that AI can be useful in improving the results of learning processes. But it is crucial that the researchers are trained well and are put in a position to doubt the outcome produced by the AI. Thus, the purpose of the paper is to discuss how AI is being used in academia at the moment and what could be done to expand its use in the future.

========================================

Our study focuses on developing an innovative system for classifying intentions in machine learning articles on arXiv. This research aims to bring a new dimension to the analysis of academic content by identifying and classifying the underlying intentions of authors, a dimension not covered by traditional search systems. Our method, which enriches content with contextual classifications, transcends simple domain categorization by incorporating specific intention identifications such as proposing new methods, analyzing existing data, or presenting experimental results. To achieve this, we have implemented natural language processing models based on transformers, which have proven to be extremely effective for this task, offering high accuracy in intention classification. The methods used and the results obtained demonstrate the effectiveness of these advanced models in enriching academic articles, thus contributing to the creation of a valuable and contextual tool for the scientific community, particularly in the field of machine learning.

========================================

Recent advances in the natural language processing (NLP) field have achieved impressive results in various tasks. However, NLP techniques are underrepresented in the analysis of Humanities and Social Science texts and in languages other than English. In particular, academic books are a highly valuable source of information that has not been exploited by these techniques at all. The recognition of named entities (person names, organizations or locations) and their semantic annotation over books could enrich the visibility and discoverability of the information by users. This is an opportunity for academia and the academic publishing industry in which semantic search is a central task and now books can be queried by named entities of interest that are in their content. This work proposes a methodology to apply named‐entity recognition to publish the results into an ontological semantic‐web format. The work has been performed over a corpus of academic books provided by UNE (Unión de Editoriales Universitarias Españolas, Union of Spanish University Presses). Results show an enrichment of the information extracted over the books and of the possibilities of querying them at the individual level but also within the whole set of books, increasing the possibilities for books to be discovered or retrieved beyond metadata.

========================================

: The advent of large language models (LLMs) has revolutionized various domains of natural language processing, including bibliographic data retrieval. This paper explores the potential of LLMs to enhance the accuracy and efficiency of retrieving bibliographic data from vast digital repositories. By leveraging the deep learning capabilities of LLMs, we propose a novel approach that surpasses traditional keyword-based search methods. Our methodology involves fine-tuning pre-trained LLMs on a comprehensive dataset of bibliographic records, enabling the model to understand and interpret complex queries more effectively. Experimental results demonstrate that our approach significantly improves precision and recall metrics, thereby reducing the retrieval of irrelevant data and enhancing the overall user experience. Furthermore, we discuss the implications of these findings for academic research, library sciences, and digital archiving, highlighting the transformative potential of LLMs in organizing and accessing scholarly information. This study provides a foundation for future research into the integration of LLMs with bibliographic databases, aiming to develop smarter, more intuitive information retrieval systems.

========================================

Language models’ growing role in natural language processing neces- sitates a deeper understanding of their linguistic knowledge. Linguistic probing tasks have become crucial for model explainability, designed to evaluate models’ understanding of vari-ous linguistic phenomena. Objective: This systematic review critically assesses the linguistic knowledge of language models via linguistic probing, providing a comprehensive overview ofthe understood linguistic phenomena and identifying future research areas. Method: We performed an extensive search of relevant academic databases and analyzed 57 articles pub- lished between October 2018 and October 2022. Results: While language models exhibit extensive linguistic knowledge, limitations persist in their comprehension of specific phe- nomena. The review also points to a need for consensus on evaluating language models’ linguistic knowledge and the linguistic terminology used. Conclusion: Our review offers an extensive look into linguistic knowledge of language models through linguistic probing tasks. This study underscores the importance of understanding these models’ linguistic capabilities for effective use in NLP applications and for fostering more explainable AI systems.

========================================

Biomedical information retrieval has often been studied as a task of detecting whether a system correctly detects entity spans and links these entities to concepts from a given terminology. Most academic research has focused on evaluation of named entity recognition (NER) and entity linking (EL) models which are key components to recognizing diseases and genes in PubMed abstracts. In this work, we perform a fine-grained evaluation intended to understand the efficiency of state-of-the-art BERT-based information extraction (IE) architecture as a biomedical search engine. We present a novel manually annotated dataset of abstracts for disease and gene search. The dataset contains 23K query-abstract pairs, where 152 queries are selected from logs of our target discovery platform and PubMed abstracts annotated with relevance judgments. Specifically, the query list also includes a subset of concepts with at least one ambiguous concept name. As a base-line, we use off-she-shelf Elasticsearch with BM25. Our experiments on NER, EL, and retrieval in a zero-shot setup show the neural IE architecture shows superior performance for both disease and gene concept queries.

========================================

In this paper, we develop a smart topics finding system to organize the research topics into a hierarchical tree. First of all, we use some natural language processing techniques to convert the collected snippets into a series of meaningful candidate terms. Second, we use a suffix tree clustering with threshold and two-steps hash to generate the topic label. Third, we use a divisive hierarchical clustering method to arrange the topic label into a hierarchical tree. In this paper, we use precision and normalized Google distance to measure the quality of topic results. According to the results of experiments, we conclude that using our system can give significant performance gains than current academic systems.

========================================

This comprehensive research paper examines the integration of Artificial Intelligence (AI) in search and recommendation systems, focusing on developments. The study delves into various AI techniques, including machine learning algorithms, natural language processing, and deep learning, and their applications in enhancing search procedures and recommendation systems. Through an extensive literature review, analysis of case studies, and examination of current challenges, this paper provides in-depth insights into the state-of-the-art AI-driven search and recommendation procedures. The research also discusses ethical considerations, future trends, and potential innovations in this rapidly evolving field, offering a holistic view of the subject matter for both industry professionals and academic researchers.

========================================

PURPOSE The Electronic Medical Record Search Engine (EMERSE) is a software tool built to aid research spanning cohort discovery, population health, and data abstraction for clinical trials. EMERSE is now live at three academic medical centers, with additional sites currently working on implementation. In this report, we describe how EMERSE has been used to support cancer research based on a variety of metrics. METHODS We identified peer-reviewed publications that used EMERSE through online searches as well as through direct e-mails to users based on audit logs. These logs were also used to summarize use at each of the three sites. Search terms for two of the sites were characterized using the natural language processing tool MetaMap to determine to which semantic types the terms could be mapped. RESULTS We identified a total of 326 peer-reviewed publications that used EMERSE through August 2019, although this is likely an underestimation of the true total based on the use log analysis. Oncology-related research comprised nearly one third (n = 105; 32.2%) of all research output. The use logs showed that EMERSE had been used by multiple people at each site (nearly 3,500 across all three) who had collectively logged into the system > 100,000 times. Many user-entered search queries could not be mapped to a semantic type, but the most common semantic type for terms that did match was “disease or syndrome,” followed by “pharmacologic substance.” CONCLUSION EMERSE has been shown to be a valuable tool for supporting cancer research. It has been successfully deployed at other sites, despite some implementation challenges unique to each deployment environment.

========================================

Background and objective Accurate identification and categorization of injuries from medical records can be challenging, yet it is important for injury epidemiology and prevention efforts. Coding systems such as the International Classification of Diseases (ICD) have well-known limitations. Utilizing computer-based techniques such as natural language processing (NLP) can help augment the identification and categorization of diseases in electronic health records. We used a Python program to search the text to identify cases of scooter injuries that presented to our emergency department (ED). Materials and methods This retrospective chart review was conducted between March 2017 and June 2019 in a single, urban academic ED with approximately 80,000 annual visits. The physician documentation was stored as combined PDF files by date. A Python program was developed to search the text from 186,987 encounters to find the string “scoot” and to extract the 100 characters before and after the phrase to facilitate a manual review of this subset of charts. Results A total of 890 charts were identified using the Python program, of which 235 (26.4%) were confirmed as e-scooter cases. Patients had an average age of 36 years and 53% were male. In 81.7% of cases, the patients reported a fall from the scooter and only 1.7% reported wearing a helmet during the event. The most commonly injured body areas were the upper extremity (57.9%), head (42.1%), and lower extremity (36.2%). The most frequently consulted specialists were orthopedic and trauma surgeons with 28% of cases requiring a consult. In our population, 9.4% of patients required admission to the hospital. Conclusions The number of results and data returned by the Python program was easy to manage and made it easier to identify charts for abstraction. The charts obtained allowed us to understand the nature and demographics of e-scooter injuries in our ED. E-scooters continue to be a popular mode of transportation, and understanding injury patterns related to them may inform and guide opportunities for policy and prevention.

========================================

USU Repository is an institutional digital information system provided by Universitas Sumatera Utara (USU) that preserves and distributes academic papers, such as thesis and dissertation, from all departments in USU. A search box is provided to help search relevant topics from this repository. However, sometimes the search results returned were irrelevant and did not satisfy the user’s expectations. One of the causes for this situation is that the search engine could not perform optimally, particularly if the query were long and complicated. One approach that can be used to solve the problem is using semantic search. Semantic search is an information retrieval process from a sentence that involves understanding the results returned by natural language processing. In light of this approach, this paper aimed to propose a semantic search to seek the relevance between the user’s input query and academic papers returned as search results. This study implemented word2vec method in converting sentences into vectors. This study indicated average precision scores for small datasets as 46% and 73% for larger datasets.

========================================

Text-based open-ended questions in academic formative and summative assessments help students become deep learners and prepare them to understand concepts for a subsequent conceptual assessment. However, grading text-based questions, especially in large courses, is tedious and time-consuming for instructors. Text processing models continue progressing with the rapid development of Artificial Intelligence (AI) tools and Natural Language Processing (NLP) algorithms. Especially after breakthroughs in Large Language Models (LLM), there is immense potential to automate rapid assessment and feedback of text-based responses in education. This systematic review adopts a scientific and reproducible literature search strategy based on the PRISMA process using explicit inclusion and exclusion criteria to study text-based automatic assessment systems in post-secondary education, screening 838 papers and synthesizing 93 studies. To understand how text-based automatic assessment systems have been developed and applied in education in recent years, three research questions are considered. All included studies are summarized and categorized according to a proposed comprehensive framework, including the input and output of the system, research motivation, and research outcomes, aiming to answer the research questions accordingly. Additionally, the typical studies of automated assessment systems, research methods, and application domains in these studies are investigated and summarized. This systematic review provides an overview of recent educational applications of text-based assessment systems for understanding the latest AI/NLP developments assisting in text-based assessments in higher education. Findings will particularly benefit researchers and educators incorporating LLMs such as ChatGPT into their educational activities.

========================================

Conversational systems have improved dramatically recently, and are receiving increasing attention in academic literature. These systems are also becoming adapted in E-Commerce due to increased integration of E-Commerce search and recommendation source with virtual assistants such as Alexa, Siri, and Google assistant. However, significant research challenges remain spanning areas of dialogue systems, spoken natural language processing, human-computer interaction, and search and recommender systems, which all are exacerbated with demanding requirements of E-Commerce. The purpose of this workshop is to bring together researchers and practitioners in the areas of conversational systems, human-computer interaction, information retrieval, and recommender systems. Bringing diverse research areas together into a single workshop would accelerate progress on adapting conversation systems to the E-Commerce domain, to set a research agenda, to examine how to build and share data sets, and to establish common evaluation metrics and benchmarks to drive research progress.

========================================

The automatic speech identification in Arabic tweets has generated substantial attention among academics in the fields of text mining and natural language processing (NLP). The quantity of studies done on this subject has experienced significant growth. This study aims to provide an overview of this field by conducting a systematic review of literature that focuses on automatic hate speech identification, particularly in the Arabic language. The goal is to examine the research trends in Arabic hate speech identification and offer guidance to researchers by highlighting the most significant studies published between 2018 and 2023. This systematic study addresses five specific research questions concerning the types of the Arabic language used, hate speech categories, classification techniques, feature engineering techniques, performance metrics, validation methods, existing challenges faced by researchers, and potential future research directions. Through a comprehensive search across nine academic databases, 24 studies that met the predefined inclusion criteria and quality assessment were identified. The review findings revealed the existence of many Arabic linguistic varieties used in hate speech on Twitter, with modern standard Arabic (MSA) being the most prominent. In identification techniques, machine learning categories are the most used technique for Arabic hate speech identification. The result also shows different feature engineering techniques used and indicates that N-gram and CBOW are the most used techniques. F1-score, precision, recall, and accuracy were also identified as the most used performance metric. The review also shows that the most used validation method is the train/test split method. Therefore, the findings of this study can serve as valuable guidance for researchers in enhancing the efficacy of their models in future investigations. Besides, algorithm development, policy rule regulation, community management, and legal and ethical consideration are other real-world applications that can be reaped from this research.

========================================

Rapid progress in the capabilities of machine learning approaches in natural language processing has culminated in the rise of large language models over the last two years. Recent works have shown unprecedented adoption of these for academic writing, especially in some fields, but their pervasiveness in astronomy has not been studied sufficiently. To remedy this, we extract words that ChatGPT uses more often than humans when generating academic text and search a total of 1 million articles for them. This way, we assess the frequency of word occurrence in published works in astronomy tracked by the NASA Astrophysics Data System since 2000. We then perform a statistical analysis of the occurrences. We identify a list of words favoured by ChatGPT and find a statistically significant increase for these words against a control group in 2024, which matches the trend in other disciplines. These results suggest a widespread adoption of these models in the writing of astronomy papers. We encourage organisations, publishers, and researchers to work together to identify ethical and pragmatic guidelines to maximise the benefits of these systems while maintaining scientific rigour.

========================================

Artificial intelligence (AI) code generation tools are crucial in software development, processing natural language to improve programming efficiency. Their increasing integration in various industries highlights their potential to transform the way programmers approach and execute software projects. The present research was conducted with the purpose of determining the accuracy and quality of code generated by artificial intelligence (AI) tools. The study began with a systematic mapping of the literature to identify applicable AI tools. Databases such as ACM, Engineering Source, Academic Search Ultimate, IEEE Xplore and Scopus were consulted, from which 621 papers were initially extracted. After applying inclusion criteria, such as English-language papers in computing areas published between 2020 and 2024, 113 resources were selected. A further screening process reduced this number to 44 papers, which identified 11 AI tools for code generation. The method used was a comparative study in which ten programming exercises of varying levels of difficulty were designed and the results obtained from 4 of them are presented. The identified tools generated code for these exercises in different programming languages. The quality of the generated code was evaluated using the SonarQube static analyzer, considering aspects such as safety, reliability and maintainability. The results showed significant variations in code quality among the AI tools. Bing as a code generation tool showed slightly superior performance compared to others, although none stood out as a noticeably superior AI. In conclusion, the research evidenced that, although AI tools for code generation are promising, they still require a pilot to reach their full potential, giving evidence that there is still a long way to go.

========================================

Within the expansive domain of “Natural Language Processing” (NLP), the task of “text summarization” emerges as a foundational element, playing a pivotal role in distilling relevant information from extensive textual corpora. In the digital age, the importance of efficient summarization becomes increasingly critical, given the overwhelming volume of textual information. This comprehensive study delves into the intricacies of both extractive and abstractive summarization techniques, placing a specific focus on transformer-based models like BERT and GPT. These models, celebrated for their remarkable capabilities in context comprehension and coherent summarization, are rigorously evaluated alongside established methods like TF-IDF, TextRank, Sumy, Fine Tuning Transformers, Model-T5, LSTM, greedy, and beam search. The practical implications of text summarization extend across diverse fields, encompassing news stories, academic papers, and social media content, underscoring its broad utility in various domains. This study not only incorporates cutting-edge models but also explores a gamut of evaluation methods to discern the quality of summarization. By intertwining theory and application, this research positions itself at the forefront of evolving summarization approaches, shedding light on the transformative impact on information consumption patterns. The dynamic landscape of summarization methods underscores the need for continuous research and innovation, as technological advancements continue to reshape how individuals access and comprehend information.

========================================

In an era marked by a surge in Indian students pursuing higher education abroad, “GradExplore” emerges as a transformative platform, addressing the intricate challenges faced by these aspiring scholars. The platform helps people looking for international education in three steps, considering their specific needs. The first phase, University Recommendation, employs a sophisticated blend of the K-Nearest Neighbors (KNN) algorithm and content-based filtering. This dual methodology ensures that recommendations are not only personalized but also align precisely with students' academic and personal preferences. Moving on to the second phase, Admission Prediction, GradExplore delves into the intricacies of ensemble learning techniques. By leveraging AdaBoost alongside gradient boosting, linear regression, logistic regression, and decision trees, the platform elevates predictive accuracy. In the third phase, Accommodation Assistance, GradExplore leverages the power of Natural Language Processing (NLP) to streamline the housing search process. GradExplore aims to enhance the international student experience, fostering enriching academic journeys and global perspectives.

========================================

In recent years, gesture recognition and speech recognition, as important input methods in Human–Computer Interaction (HCI), have been widely used in the field of virtual reality. In particular, with the rapid development of deep learning, artificial intelligence, and other computer technologies, gesture recognition and speech recognition have achieved breakthrough research progress. The search platform used in this work is mainly the Google Academic and literature database Web of Science. According to the keywords related to HCI and deep learning, such as “intelligent HCI”, “speech recognition”, “gesture recognition”, and “natural language processing”, nearly 1000 studies were selected. Then, nearly 500 studies of research methods were selected and 100 studies were finally selected as the research content of this work after five years (2019–2022) of year screening. First, the current situation of the HCI intelligent system is analyzed, the realization of gesture interaction and voice interaction in HCI is summarized, and the advantages brought by deep learning are selected for research. Then, the core concepts of gesture interaction are introduced and the progress of gesture recognition and speech recognition interaction is analyzed. Furthermore, the representative applications of gesture recognition and speech recognition interaction are described. Finally, the current HCI in the direction of natural language processing is investigated. The results show that the combination of intelligent HCI and deep learning is deeply applied in gesture recognition, speech recognition, emotion recognition, and intelligent robot direction. A wide variety of recognition methods were proposed in related research fields and verified by experiments. Compared with interactive methods without deep learning, high recognition accuracy was achieved. In Human–Machine Interfaces (HMIs) with voice support, context plays an important role in improving user interfaces. Whether it is voice search, mobile communication, or children’s speech recognition, HCI combined with deep learning can maintain better robustness. The combination of convolutional neural networks and long short-term memory networks can greatly improve the accuracy and precision of action recognition. Therefore, in the future, the application field of HCI will involve more industries and greater prospects are expected.

========================================

Seq2Seq models and their variants have become a mainstay of modern natural language processing and sequence modelling tasks. Just Information about Seq2Seq models. In this paper, we provide a comprehensive overview of the evolution of Seq2Seq architecture from early-stage RNN based approaches to recent Transformer based methods. The paper extensively covers additional important methods such as attention mechanisms, bidirectional encoders, pointer-generator networks, as well as optimization methods such as beam search, scheduled sampling and reinforcement learning. It also discusses the challenges of data preprocessing, loss functions, and evaluation metrics, as well as applications in machine translation, summarization, speech recognition, and conversational AI. This paper provides a comprehensive report on the design and future directions of Seq2Seq models emphasizing on theoretical foundations as well as real world applications.

========================================

ChatGPT an artificial intelligence (AI) program based on natural language processing and deep learning algorithm training, has the capacity to learn human languages and understand their semantics through colossal amounts of text data and to conduct natural language-based communication with humans (Jiao, 2023). It has garnered tremendous attention of the internet users since its release in 2022. It has also instigated a lot of discussions on its application in education among educational researchers and teachers. According to Aljanabi (2023), ChatGPT’s potent information search and organization capabilities enable the student to seek out a more complete answer for the question and to obtain learning resources needed using natural language. Chu (2023) argued that ChatGPT could act as an AI teaching assistant to aid teachers in developing course plans and generating questions of varying difficulty levels as well as an intelligent academic supporter to help students with their academic papers and assessments. Research also shows that ChatGPT has significant value in the area of personalized learning.

========================================

The way people learn has undergone a significant transformation thanks to the advancement of technology. Various digital tools complement daily and academic activities, facilitating access to updated and diverse information. The Internet, in particular, has positioned itself as the primary source of information, offering a large amount of textual and audiovisual content, with short content in the form of videos being the most popular. However, the learning process inevitably involves acquiring new concepts and terms. When encountering unfamiliar vocabulary in videos, people often search for additional information to understand the content better. Therefore, this research seeks to develop a tool capable of analyzing video transcripts using Natural Language Processing techniques to identify key terms and relate them to other relevant information sources, thus facilitating learning. When evaluating the relevance of terms to the textual content of videos on various topics using an Artificial Intelligence model, a relevance greater than 75% was evidenced for all terms. This confirms the efficacy of this approach for analyzing and understanding the textual content of transcribed videos.

========================================

Sentiment analysis (SA) is the process of Finding or adding a meaning to a text using natural language processing, a deep examination of data that is maintained online at various platforms such as movie reviews, product reviews at different online shopping apps, twitter (now X), etc., to recognize and classify the opinions and ideas that are expressed in a certain passage of text. The process's aim is to search for the author's perspective on a certain topic, film, item, etc. The outcome can be neutral, bad, or good. There have been studies that demonstrated various methods in the SA methodology for gathering and analysing feelings related to the selected subjects’ polarity—positive, negative, or neutral. Social media SA is a reliable source of data and information. SA becomes significant in a variety of business, social, and academic fields. The evaluation and sentiment analysis process, however, is looked upon with difficulties. Finding the proper side of polarity and accurately interpreting sentiments that are hindered by these difficulties. Sentiment analysis is a part of natural language processing and used text mining to find and evaluate the bias from the text.

========================================

Academic institutions amass a wealth of knowledge through student projects, yet this valuable resource often remains
untapped. ProjectValt addresses this by creating a centralized platform that empowers users to efficiently discover, understand,
and leverage past student work.
At the core of ProjectValt lies an intelligent summarization tool. Utilizing advanced natural language processing, this tool
automatically generates concise summaries of project reports, enabling users to quickly grasp key findings and insights.
Furthermore, ProjectValt incorporates sophisticated search capabilities, including keyword matching and semantic similarity
analysis. This allows users to effectively search for projects based on specific criteria, fostering a culture of knowledge sharing
and reuse within the academic community.
By providing a user-friendly interface and powerful tools, ProjectValt unlocks the potential of past student projects. This
platform not only preserves valuable research but also inspires future innovation by enabling students, faculty, and researchers
to build upon the work of their predecessors

========================================

In the era of big data and articial intelligence, machine learning has become a crucial tool for extracting insights and making predictions across various domains. Bayes theorem, a fundamental principle in probability theory, has emerged as a cornerstone in many machine learning algorithms. This literature review explores the main applications of Bayes theorem in machine learning, focusing on its role in classication, Natural Language Processing (NLP), and other emerging elds. The study aims to provide an overview of how Bayesian principles enhance learning algorithms, improve decision-making processes, and address complex problems in articial intelligence. Through a systematic review of academic papers from Google Scholar, this research synthesizes current knowledge on Bayesian methods in machine learning. The methodology involves dening the research scope, conducting a literature search using specic keywords, screening relevant studies, and analyzing the collected data. By examining diverse applications ranging from disease prediction to sentiment analysis, this review highlights the versatility and signicance of Bayes theorem in advancing machine learning techniques and their real-world implementations.

========================================

Purpose
This paper aims to examine the evolution of DeepSeek R1 as an advanced alternative to ChatGPT in academic and library contexts. It highlights DeepSeek R1’s potential to enhance research methodologies, optimize search and metadata processing and refine content development. Additionally, it explores the ethical implications of integrating next-generation AI models in academia, focusing on bias, privacy and academic integrity.

Design/methodology/approach
The study employs a comparative analysis of DeepSeek R1 and ChatGPT, evaluating their capabilities in academic applications through structured interviews and technical assessments. Key areas analyzed include natural language processing, reasoning capabilities and code generation. The research also incorporates an ethical framework to assess the implications of AI-driven academic support tools.

Findings
DeepSeek R1 outperforms ChatGPT in several academic domains, particularly in reasoning, computational efficiency and metadata organization. Its open-source architecture allows for greater customization, making it more adaptable for academic institutions. However, the study also identifies limitations, such as content filtering restrictions and ethical concerns related to AI dependence and data privacy. The findings suggest that DeepSeek R1 can serve as a powerful complement to existing AI tools in academia.

Research limitations/implications
Given the rapid development of AI models, this study’s findings may require future validation as newer versions of DeepSeek R1 and ChatGPT emerge. Further research is recommended to explore long-term AI adoption in academic settings and assess evolving ethical concerns.

Practical implications
This research provides valuable insights for academic institutions and libraries on integrating AI-driven tools. It offers practical recommendations for leveraging DeepSeek R1 to enhance academic workflows, while ensuring ethical AI use through transparency, bias mitigation and privacy safeguards.

Originality/value
This paper contributes to the discourse on AI in academia by providing an in-depth comparative analysis of DeepSeek R1 and ChatGPT. It offers a structured evaluation of AI’s evolving role in academic research and library services, emphasizing ethical considerations and best practices for AI adoption in educational institutions.


========================================

Abstracting, Documenting, and Archiving Network (ADAN) system was developed to address the challenges of scientific information management in the digital era by integrating abstraction, documentation, and archiving functions in one integrated platform. This study aims to design an efficient and responsive system to academic needs by utilizing artificial intelligence (AI), natural language processing (NLP), and machine learning (ML) technologies. In-depth literature study and needs analysis through interviews, Focus Group Discussions (FGD), and surveys of 250 academic respondents identified priorities such as fast access, data preservation, and content recommendations. ADAN is designed with a modular 5-tier architecture, supporting interoperability with standards such as Dublin Core and OAI-PMH. Test results show that automatic metadata extraction achieves 93% accuracy, 89% content classification, and 0.8 seconds search response time for 10,000 documents. The system also offers an intuitive user interface with a System Usability Scale (SUS) score of 82.5, confirming its potential as an innovative solution in scientific information management.

========================================


The project "Keyword-Based Exploration of Library Resources" addresses the challenges associated with

accessing and discovering academic resources efficiently. Traditional systems often suffer from limitations such as inadequate multilingual support, poor metadata utilization, and restricted filtering capabilities, which hinder users from locating relevant research materials effectively. This project proposes an innovative solution

leveraging Artificial Intelligence (AI) and Natural Language Processing (NLP) techniques to enhance search capabilities and inclusivity.

The system incorporates:

•  Multilingual Search: Enabling users to perform queries in various languages using translation APIs.

•  Advanced Filtering Options: Allowing searches to be refined by author, publication year, journal, and more.

•  AI-Powered Metadata Extraction: Utilizing Optical Character Recognition (OCR) and NLP to extract and catalogue metadata like keywords, authors, and publication years.

The proposed system is built on a Python backend using Flask for API integration and MyAWS CLOUD for secure data storage. By integrating robust search mechanisms and user-friendly design, the project contributes to Sustainable Development Goal 4 (Quality Education), fostering global accessibility to knowledge and academic research. The outcomes of this project are anticipated to significantly improve resource discoverability,

inclusivity, and precision, addressing the needs of diverse academic communities.

INDEX TERMS
Keyword Search, Library Resource Management, Information Retrieval, Digital Libraries, Metadata Extraction, Search Optimization, Natural Language Processing (NLP), Database Searching, Search Algorithms, Document Retrieval Systems, Academic Research Tools.

========================================

Finding experts in any field can be difficult, especially when relying on academic publications given the use of jargons despite publication lists being publicly available. Within the use of publications, discernment of expertise by authorship positions is often absent in the many pub-lication-based expert search platforms available which could function as another discernment filter of expertise. Given that it is common in many academic fields for the research group lead or lab heads to take the position of the last author, the existing authorship scoring systems that assign a decreasing weightage from the first author would not reflect the last author correctly. To address these mentioned problems, we incorporated natural language processing (Common Crawl using fastText) to identify related keywords for a search compatible to using jargons as well as an au-thorship positional scoring with the option to provide greater weightage to the last author. The resulting output is a ranked scoring system of researchers upon every search which we imple-mented as a webserver for internal agency use called ‘APD lab Capability & Expertise Search (ACES)’ webserver which can be accessed from webserver.apdskeg.com/aces.

========================================

Despite the public availability, finding experts in any field when relying on academic publications can be challenging, especially with the use of jargons. Even after overcoming these issues, the discernment of expertise by authorship positions is often also absent in the many publication-based search platforms. Given that it is common in many academic fields for the research group lead or lab head to take the position of the last author, some of the existing authorship scoring systems that assign a decreasing weightage from the first author would not reflect the last author correctly. To address these problems, we incorporated natural language processing (Common Crawl using fastText) to retrieve related keywords when using jargons as well as a modified authorship positional scoring that allows the assignment of greater weightage to the last author. The resulting output is a ranked scoring system of researchers upon every search that we implemented as a webserver for internal use called the APD lab Capability & Expertise Search (ACES).

========================================

We propose a multitask pretraining approach ZeroPrompt for zero-shot generalization, focusing on task scaling and zero-shot prompting. While previous models are trained on only a few dozen tasks, we scale to 1,000 tasks for the first time using real-world data. This leads to a crucial discovery that task scaling can be an efficient alternative to model scaling; i.e., the model size has little impact on performance with an extremely large number of tasks. Our results show that task scaling can substantially improve training efficiency by 30 times in FLOPs. Moreover, we present a prompting method that incorporates a genetic algorithm to automatically search for the best prompt for unseen tasks, along with a few other improvements. Empirically, ZeroPrompt substantially improves both the efficiency and the performance of zero-shot learning across a variety of academic and production datasets.

========================================

The success of research institutions heavily relies upon identifying the right researchers"for the job": researchers may need to identify appropriate collaborators, often from across disciplines; students may need to identify suitable supervisors for projects of their interest; administrators may need to match funding opportunities with relevant researchers, and so on. Usually, finding potential collaborators in institutions is a time-consuming manual search task prone to bias. In this paper, we propose a novel query-based framework for searching, scoring, and exploring research expertise automatically, based upon processing abstracts of academic publications. Given user queries in natural language, our framework finds researchers with relevant expertise, making use of domain-specific knowledge bases and word embeddings. It also generates explanations for its recommendations. We evaluate our framework with an institutional repository of papers from a leading university, using, as baselines, artificial neural networks and transformer-based models for a multilabel classification task to identify authors of publication abstracts. We also assess the cross-domain effectiveness of our framework with a (separate) research funding repository for the same institution. We show that our simple method is effective in identifying matches, while satisfying desirable properties and being efficient.

========================================

Objective To provide a state of the art review of artificial intelligence (AI), including its subfields of machine learning and natural language processing, as it applies to otolaryngology and to discuss current applications, future impact, and limitations of these technologies. Data Sources PubMed and Medline search engines. Review Methods A structured search of the current literature was performed (up to and including September 2018). Search terms related to topics of AI in otolaryngology were identified and queried to identify relevant articles. Conclusions AI is at the forefront of conversation in academic research and popular culture. In recent years, it has been touted for its potential to revolutionize health care delivery. Yet, to date, it has made few contributions to actual medical practice or patient care. Future adoption of AI technologies in otolaryngology practice may be hindered by misconceptions of what AI is and a fear that machine errors may compromise patient care. However, with potential clinical and economic benefits, it is vital for otolaryngologists to understand the principles and scope of AI. Implications for Practice In the coming years, AI is likely to have a major impact on biomedical research and the practice of medicine. Otolaryngologists are key stakeholders in the development and clinical integration of meaningful AI technologies that will improve patient care. High-quality data collection is essential for the development of AI technologies, and otolaryngologists should seek opportunities to collaborate with data scientists to guide them toward the most impactful clinical questions.

========================================

The interpretation of variation in the human genome constitutes one of the most pressing challenges in biomedicine. There are many academic and proprietary resources that provide annotation, interpretation, scoring and knowledge of genetic variants under multiple access modalities. For these resources, information is available through portals, wikis, APIs, curated content and databases, and pipelines. Here we explore the use of search motors to provide facilitated access to the main sources of information that are used by clinical geneticists and researchers. We also support the browsing experience with natural language processing and automated summaries of available information. The resulting tool, ai-OMNI.com, is intrinsically flexible, expandable and intuitive, thus providing a different experience for querying the human genome for the consequences of variation.

========================================

Objective At the forefront of machine learning research since its inception has been natural language processing, also known as text mining, referring to a wide range of statistical processes for analyzing textual data and retrieving information. In medical fields, text mining has made valuable contributions in unexpected ways, not least by synthesizing data from disparate biomedical studies. This rapid scoping review examines how machine learning methods for text mining can be implemented at the intersection of these disparate fields to improve the workflow and process of conducting systematic reviews in medical research and related academic disciplines. Methods The primary research question that this investigation asked, “what impact does the use of machine learning have on the methods used by systematic review teams to carry out the systematic review process, such as the precision of search strategies, unbiased article selection or data abstraction and/or analysis for systematic reviews and other comprehensive review types of similar methodology?” A literature search was conducted by a medical librarian utilizing multiple databases, a grey literature search and handsearching of the literature. The search was completed on December 4, 2020. Handsearching was done on an ongoing basis with an end date of April 14, 2023. Results The search yielded 23,190 studies after duplicates were removed. As a result, 117 studies (1.70%) met eligibility criteria for inclusion in this rapid scoping review. Conclusions There are several techniques and/or types of machine learning methods in development or that have already been fully developed to assist with the systematic review stages. Combined with human intelligence, these machine learning methods and tools provide promise for making the systematic review process more efficient, saving valuable time for systematic review authors, and increasing the speed in which evidence can be created and placed in the hands of decision makers and the public.

========================================

Dependency parsers are among the most crucial tools in natural language processing as they have many important applications in downstream tasks such as information retrieval, machine translation and knowledge acquisition. We introduce the Yara Parser, a fast and accurate open-source dependency parser based on the arc-eager algorithm and beam search. It achieves an unlabeled accuracy of 93.32 on the standard WSJ test set which ranks it among the top dependency parsers. At its fastest, Yara can parse about 4000 sentences per second when in greedy mode (1 beam). When optimizing for accuracy (using 64 beams and Brown cluster features), Yara can parse 45 sentences per second. The parser can be trained on any syntactic dependency treebank and dierent options are provided in order to make it more flexible and tunable for specific tasks. It is released with the Apache version 2.0 license and can be used for both commercial and academic purposes. The parser can be found at https: //github.com/yahoo/YaraParser.

========================================

Anti-neutrophil cytoplasmatic antibody (ANCA) associated vasculitis (AAV) is a rare, life-threatening, systemic auto-immune disease.[1]Due to the low prevalence, multiple treating disciplines and poor registration, including ICD-10 classification, identifying AAV patients for (pre-)clinical studies, research and health care evaluation is challenging.[2, 3]Therefore, there is an urgent need to improve identifying these patients in health care organisations. Employing artificial intelligence (AI) – supported search engines are increasingly suggested to achieve this.Reliably identify AAV patients in electronic health records (EHR) using an AI-based tool that incorporates text mining and Natural Language Processing (NLP).The identification method consists of a search strategy combined with NLP-based exclusion. A search strategy to optimally identify AAV patients within a single center EHR system of an academic hospital was developed using an established AAV cohort (n=203) as a gold-standard reference set. Patient records, identified by the search strategy outside of the reference set, underwent manual review to confirm newly-identified AAV patients. Then, improvement in performance by adding NLP to the text-mining search strategy was investigated. Validation was performed on an independent EHR system of a non-academic hospital with an established AAV cohort available for reference (n=84).The search strategy combined five queries based on disease description, laboratory measurements, medication and relevant specialisms. In the determination EHR, the search strategy identified 608 patients, including 197/203 (97.0%) AAV patients of the reference set and 149 newly-identified AAV cases confirmed by manual review. Employing NLP in the identification method improved the positive predictive value (PPV) from 57% (346/608 patients) to 78% (339/444 patients). These results were validated in an independent EHR system where the search strategy identified 333 patient, including 82/84 (97.6%) AAV patients of the reference set and 112 newly-identified AAV cases upon manual review. NLP improved PPV from 59% (194/333 patients) to 86% (192/223 patients). Negative predictive values and specificities were above 98% in all analyses.We present an AI based identification method to identify low-prevalent AAV patients in EHR systems. We demonstrated improved performance when adding NLP to the text-mining search strategy. Successful validation in an independent health organisation supports the applicability and transportability of this method which can be an important accelerator for research efforts and health care evaluation in AAV patients.[1]Kitching AR, Anders HJ, Basu N, Brouwer E, Gordon J, Jayne DR, et al. ANCA-associated vasculitis. Nat Rev Dis Primers. 2020;6(1):71.[2]Quan H, Li B, Saunders LD, Parsons GA, Nilsson CI, Alibhai A, et al. Assessing validity of ICD-9-CM and ICD-10 administrative data in recording clinical conditions in a unique dually coded database. Health Serv Res. 2008;43(4):1424-41.[3]Spierings J BT, Dirikgil E, Moens HB. Overdaad aan ICD-coderingen hindert onderzoek. Medisch Contact. 2020;22:18-20.NIL.Jolijn van Leeuwen: None declared, Erik Penne: None declared, Y.K. Onno Teng Consultant of: YKOT received an unrestricted research grant and consultancy fees from Vifor Pharma., Grant/research support from: The work of YKOT was supported by the Dutch Kidney Foundation (17OKG04) and by the Arthritis Research and Collaboration Hub Foundation. Arthritis Research and Collaboration Hub is funded by Dutch Arthritis Foundation (ReumaNederland).

========================================

In the field of natural language processing (NLP), keywords are crucial for enhancing information retrieval (IR) and content summarization, as well as for optimizing search engines and organizing documents. As the volume of generated information increases, identifying keywords manually from large documents becomes more challenging and no longer feasible. Therefore, automatic keyword extraction is necessary as a cost-effective method to handle large documents and to provide scalable solutions for various applications in NLP and information management. In the academic domain, automatic keyword extraction simplifies the process of finding and categorizing scientific publications, enabling paper repositories to optimize their IR and document organizing systems. However, many methods of keyword extraction use either global semantic features based on pre-trained embedding models or local statistical features separately, which yields low results. Since a good keyword must be identified by both external knowledge and local statistical features, this paper proposes a method to improve the performance of keyword extraction from scientific publications by combining local statistical features with embedding models. The proposed method outperforms the baseline methods with an F-score of 0.70 on the SemEval2017 dataset using the SciBERT model and SVM classifier. This research confirms that both local statistical information and contextualized semantic information are important to identify keywords.

========================================

The process of conducting literature reviews is often time-consuming and labor-intensive. To streamline this process, I present an AI Literature Review Suite that integrates several functionalities to provide a comprehensive literature review. This tool leverages the power of open access science, large language models (LLMs) and natural language processing to enable the searching, downloading, and organizing of PDF files, as well as extracting content from articles. Semantic search queries are used for data retrieval, while text embeddings and summarization using LLMs present succinct literature reviews. Interaction with PDFs is enhanced through a user-friendly graphical user interface (GUI). The suite also features integrated programs for bibliographic organization, interaction and query, and literature review summaries. This tool presents a robust solution to automate and optimize the process of literature review in academic and industrial research.

========================================

This project presents an innovative solution to facilitate the search and analysis of professional profiles in academic platforms by automating manual processes and using advanced technologies such as Natural Language Processing (NLP) and third-party application programming interface (API). We designed an architecture based on a React/NextJS front-end and back-end with REST APIs to enable robust and scalable performance for the proposed system. Our proposed architecture and implementation plan provided a solid foundation for the development of the solution. Our initial prototype and advanced functionalities, such as sentiment analysis, keyword extraction, and personalized profile recommendations, are expected to improve the user experience and provide added value to the academic and professional community.

========================================

. Background: Anti-neutrophil cytoplasmatic antibody (ANCA)-associated vasculitis (AAV) is a rare, life-threatening, systemic auto-immune disease. Due to the low prevalence and heterogenous registration, there is an urgent need to improve identification of AAV patients within the electronic health record (EHR)-system of health organizations to facilitate clinical research. Methods: Our aim was to identify, with a high sensitivity, low-prevalence AAV patients within large EHR-systems (>2.000.000 records) using an artificial intelligence (AI)-search tool. We combined a search on structured and unstructured data with natural language processing (NLP)-based exclusion. We developed the method in an academic center with an established AAV training set (n=203) and validated the method in a non-academic center with a validation set (n=84). We anonymously reviewed all identified patient records for AAV diagnosis. Results: The final search strategy combined four queries on disease description, laboratory measurements, medication and specialisms. In the training center, this search identified 608 patients, of which 346 were AAV patients upon manual review. 197/203 patients of the training set were retrieved, indicating a sensitivity of 97%. Employing NLP-based exclusion resulted in 444 patients with 339 AAV patients, resulting in an increase of positive predictive value (PPV) from 57% to 78% and a sensitivity of 96%. In the validation center the search strategy identified 333 patients, of which 194 were AAV patients, including 82/84 (98%) patients of the validation set. After NLP-based exclusion 223 patients remained, including 196 AAV patients, improving PPV from 58 to 86% with a sensitivity of 98%. Our identification method outperformed ICD-10 coding predominantly in identifying myeloperoxidase (MPO)-positive AAV patients and patients with few specialisms involved. Conclusions: We demonstrated excellent performance of an AI-based identification method, incorporating NLP, to identify AAV patients in EHRs and we validated the applicability and transportability. This method can accelerate research efforts, while avoiding the limitations of ICD-10-based registration.

========================================

Natural Language Processing (NLP) has advanced recently, and this has fundamentally changed how humans interact with and understand written material. Among the many uses of natural language processing (NLP), automated question generation (QG) has attracted significant interest in data retrieval and academic research. This study explores novel approaches and tactics for bringing NLP methods to the field of QG. This study provides a thorough synopsis of the many stages of the QG process, including the preparation of preliminary data, the creation of questionnaires, and the interviewing procedure. This study examines each of these phases in detail and discusses about the issues that have surfaced recently along with their associated fixes. It also highlights the critical role that neural networks—in particular, transformer-based models—play in improving QG system performance. Furthermore, this study covers a wide range of applications for automated QG, from search engine building and chatbot interaction to the creation of content for research and education. give case studies and practical applications where the effectiveness of automated QG has been shown. The study also looks at accepted assessment measures and benchmarks for determining the quality of questionnaires that are created. To make it possible to compare QG systems meaningfully, this study emphasizes the value of human participation in research and the usefulness of standardized datasets.

========================================

. Background: Anti-neutrophil cytoplasmatic antibody (ANCA)-associated vasculitis (AAV) is a rare, life-threatening, systemic auto-immune disease. Due to the low prevalence and heterogenous registration, there is an urgent need to improve identification of AAV patients within the electronic health record (EHR)-system of health organizations to facilitate clinical research. Methods: Our aim was to identify, with a high sensitivity, low-prevalence AAV patients within large EHR-systems (>2.000.000 records) using an artificial intelligence (AI)-search tool. We combined a search on structured and unstructured data with natural language processing (NLP)-based exclusion. We developed the method in an academic center with an established AAV training set (n=203) and validated the method in a non-academic center with a validation set (n=84). We anonymously reviewed all identified patient records for AAV diagnosis. Results: The final search strategy combined four queries on disease description, laboratory measurements, medication and specialisms. In the training center, this search identified 608 patients, of which 346 were AAV patients upon manual review. 197/203 patients of the training set were retrieved, indicating a sensitivity of 97%. Employing NLP-based exclusion resulted in 444 patients with 339 AAV patients, resulting in an increase of positive predictive value (PPV) from 57% to 78% and a sensitivity of 96%. In the validation center the search strategy identified 333 patients, of which 194 were AAV patients, including 82/84 (98%) patients of the validation set. After NLP-based exclusion 223 patients remained, including 196 AAV patients, improving PPV from 58 to 86% with a sensitivity of 98%. Our identification method outperformed ICD-10 coding predominantly in identifying myeloperoxidase (MPO)-positive AAV patients and patients with few specialisms involved. Conclusions: We demonstrated excellent performance of an AI-based identification method, incorporating NLP, to identify AAV patients in EHRs and we validated the applicability and transportability. This method can accelerate research efforts, while avoiding the limitations of ICD-10-based registration.

========================================

Information retrieval system is taking an important role in current search engine which performs searching operation based on keywords which results in enormous amount of data available to the user, from which user cannot figure out the essential and most important information. This limitation may be overcome by a new web architecture known as semantic web which overcome the limitation of keyword based search technique called conceptual or semantic search technique. Natural language processing technique is mostly implemented in QA system for asking user's question and several steps are also followed for conversion of questions to query form for getting an exact answer. In conceptual search, search engine interprets the meaning of user's query and the relation among the concepts that documents contains with respect to a particular domain that produces specific answers instead of giving list of answers. In this paper, we proposed ontology based semantic information retrieval system and Jena semantic web framework in which, user enters an input query which is parsed by Standford Parser then triplet extraction algorithm is used. To all input query, SPARQL query is formed and then it is fired on the knowledge base (Ontology) that finds appropriate RDF triples in knowledge base and retrieve the relevant information using Jena framework.

========================================

Codification of free-text clinical narratives have long been recognised to be beneficial for secondary uses such as funding, insurance claim processing and research. The current scenario of assigning codes is a manual process which is very expensive, time-consuming and error prone. In recent years, many researchers have studied the use of Natural Language Processing (NLP), related Machine Learning (ML) and Deep Learning (DL) methods and techniques to resolve the problem of manual coding of clinical narratives and to assist human coders to assign clinical codes more accurately and efficiently. This systematic literature review provides a comprehensive overview of automated clinical coding systems that utilises appropriate NLP, ML and DL methods and techniques to assign ICD codes to discharge summaries. We have followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses(PRISMA) guidelines and conducted a comprehensive search of publications from January, 2010 to December 2020 in four academic databases- PubMed, ScienceDirect, Association for Computing Machinery(ACM) Digital Library, and the Association for Computational Linguistics(ACL) Anthology. We reviewed 7,556 publications; 38 met the inclusion criteria. This review identified: datasets having discharge summaries; NLP techniques along with some other data extraction processes, different feature extraction and embedding techniques. To measure the performance of classification methods, different evaluation metrics are used. Lastly, future research directions are provided to scholars who are interested in automated ICD code assignment. Efforts are still required to improve ICD code prediction accuracy, availability of large-scale de-identified clinical corpora with the latest version of the classification system. This can be a platform to guide and share knowledge with the less experienced coders and researchers.

========================================

The increasing volume, variety, velocity, distribution, structural intricacy, and complexity of use of digital evidence can make it difficult for practitioners to find and understand the most forensically useful information (Casey E. Digital evidence and computer crime: Forensic science, computers, and the Internet. Academic Press; 2011. p. 31; Pollitt M. The hermeneutics of the hard drive: Using narratology, natural language processing, and knowledge management to improve the effectiveness of the digital forensic process [PhD dissertation]. University of Central Florida; 2011). Digital forensic practitioners currently search for information and solutions in an ad hoc manner, leading to results that are unstructured, unverified, and sometimes incomplete. As a result, certain digital evidence is being missed or misinterpreted. To mitigate risks of knowledge gaps, there is a pressing need for a systematic mechanism that practitioners can use to codify and combine their collective knowledge. This work presents the design and development of a solution that catalogs crowdsourced knowledge of digital forensic artifacts in a well‐structured, easily searchable form to support efficient and automated extraction of pertinent information, improving availability and reliability of interpretation of artifacts (general acceptance). Technical implementation and artifact curation are discussed with illustrative examples and recommendations for future work.

========================================

Background Breast cancer is the most common malignancy to spread to the orbit and periorbit, and the invasive lobular carcinoma (ILC) histologic subtype of breast cancer has been reported to form these ophthalmic metastases (OM) more frequently than invasive ductal carcinomas (IDC). We herein report our single academic institution experience with breast cancer OM with respect to anatomical presentation, histology (lobular vs. ductal), treatment, and survival. Methods We employed the natural language processing platform, TIES (Text Information Extraction System), to search 2.3 million de-identified patient pathology and radiology records at our institution in order to identify patients with OM secondary to breast cancer. We then compared the resultant cohort, the “OM cohort,” to two other representative metastatic breast cancer patient (MBC) databases from our institution. Histological analysis of selected patients was performed. Results Our TIES search and manual refinement ultimately identified 28 patients who were diagnosed with breast cancer between 1995 and 2016 that subsequently developed OM. Median age at diagnosis was 54 (range 28–77) years of age. ER, PR, and HER2 status from the 28 patients with OM did not differ from other patients with MBC from our institution. The relative proportion of patients with ILC was significantly higher in the OM cohort (32.1%) than in other MBC patients in our institution (11.3%, p  = 0.007). Median time to first OM in the OM cohort was 46.7 months, and OM were the second most frequent first metastases after bony metastases. After diagnosis of the first distant metastasis of any kind, median survival of patients with ILC (21.4 months) was significantly shorter than that of patients with IDC (55.3 months, p  = 0.03). Nine patients developed bilateral OM. We observed a significant co-occurrence of OM and central nervous system metastases ( p  = 0.0053). The histological analysis revealed an interesting case in which the primary tumor was of a mixed ILC/IDC subtype, while only ILC was present in the OM. Conclusions OM from breast cancer are illustrative of the difference in metastatic behavior of ILC versus IDC and should be considered when treating patients with ILC, especially in those with complaints of visual acuity changes.

========================================

We present DEADLINER, a search engine that catalogs conference and workshop announcements, and ultimately will monitor and extract a wide range of academic convocation material from the web. The system currently extracts speakers, locations, dates, paper submission (and other) deadlines, topics, program committees, abstracts, and aAEliations. A user or user agent can perform detailed searches on these elds. DEADLINER was constructed using a methodology for rapid implementation of specialized search engines. This methodology avoids complex hand-tuned text extraction solutions, or natural language processing, by Bayesian integration of simple extractors that exploit loose formatting and keyw ord con ventions. The Bayesian framework further produces a search engine where each user can control the false alarm rate on a eld in an intuitive yet rigorous fashion.

========================================

. Background: Anti-neutrophil cytoplasmatic antibody (ANCA)-associated vasculitis (AAV) is a rare, life-threatening, systemic auto-immune disease. Due to the low prevalence and heterogenous registration, there is an urgent need to improve identification of AAV patients within the electronic health record (EHR)-system of health organizations to facilitate clinical research. Methods: Our aim was to identify, with a high sensitivity, low-prevalence AAV patients within large EHR-systems (>2.000.000 records) using an artificial intelligence (AI)-search tool. We combined a search on structured and unstructured data with natural language processing (NLP)-based exclusion. We developed the method in an academic center with an established AAV training set (n=203) and validated the method in a non-academic center with a validation set (n=84). We anonymously reviewed all identified patient records for AAV diagnosis. Results: The final search strategy combined four queries on disease description, laboratory measurements, medication and specialisms. In the training center, this search identified 608 patients, of which 346 were AAV patients upon manual review. 197/203 patients of the training set were retrieved, indicating a sensitivity of 97%. Employing NLP-based exclusion resulted in 444 patients with 339 AAV patients, resulting in an increase of positive predictive value (PPV) from 57% to 78% and a sensitivity of 96%. In the validation center the search strategy identified 333 patients, of which 194 were AAV patients, including 82/84 (98%) patients of the validation set. After NLP-based exclusion 223 patients remained, including 196 AAV patients, improving PPV from 58 to 86% with a sensitivity of 98%. Our identification method outperformed ICD-10 coding predominantly in identifying myeloperoxidase (MPO)-positive AAV patients and patients with few specialisms involved. Conclusions: We demonstrated excellent performance of an AI-based identification method, incorporating NLP, to identify AAV patients in EHRs and we validated the applicability and transportability. This method can accelerate research efforts, while avoiding the limitations of ICD-10-based registration.

========================================

In this paper, we present a business intelligence (BI) toolkit based on natural language processing (NLP) methods for the arts and culture domain, collected in the Measuring the Social Dimension of Culture (MESOC) project. The main NLP methods in the underlying pipeline are keyword extraction, multi-label classification of texts, and detection of potential social impacts of cultural policies and practices, all trained on texts from open-access academic publications. The MESOC Toolkit is a georeferenced visualization tool for analyzing impact on social value creation in the areas of health and well-being, urban regeneration, and social cohesion, and enables semantic search for content in the MESOC domain. Therefore, the presented research can serve as a prototype for measuring societal value by identifying recurrent pathways of transformational processes in society that reach beyond the selected field of art and culture.

========================================

The field of Natural Language Processing (NLP) is growing rapidly, with new research published daily along with an abundance of tutorials, codebases and other online resources. In order to learn this dynamic field or stay up-to-date on the latest research, students as well as educators and researchers must constantly sift through multiple sources to find valuable, relevant information. To address this situation, we introduce TutorialBank, a new, publicly available dataset which aims to facilitate NLP education and research. We have manually collected and categorized over 5,600 resources on NLP as well as the related fields of Artificial Intelligence (AI), Machine Learning (ML) and Information Retrieval (IR). Our dataset is notably the largest manually-picked corpus of resources intended for NLP education which does not include only academic papers. Additionally, we have created both a search engine and a command-line tool for the resources and have annotated the corpus to include lists of research topics, relevant resources for each topic, prerequisite relations among topics, relevant sub-parts of individual resources, among other annotations. We are releasing the dataset and present several avenues for further research.

========================================

OBJECTIVE
To introduce the research methods of computerized text mining and its possible applications in suicide research and to demonstrate the procedures of applying a specific text mining area, document classification, to a suicide-related study.


METHOD
A systematic search of academic papers that applied text mining methods to suicide research was conducted. Relevant papers were reviewed focusing on their research objectives and sources of data. Furthermore, a case of using natural language processing and document classification methods to analyze a large amount of suicide news was elaborated to showcase the methods.


RESULTS
Eighty-six papers using text mining methods for suicide research have been published since 2001. The most common research objective (72.1%) was to classify which documents exhibit suicide risk or were written by suicidal people. The most frequently used data source was online social media posts (45.3%), followed by e-healthcare records (25.6%). For the news classification case, the top three classifiers trained for classification tasks achieved 84% or higher accuracy.


CONCLUSIONS
Computerized text mining methods can help to scale up content analysis capacity and efficiency and uncover new insights and perspectives for suicide research.

========================================

The Bibliometric-enhanced Information Retrieval workshop series (BIR) was launched at ECIR in 2014 [19] and it was held at ECIR each year since then. This year we organize the 10th iteration of BIR. The workshop series at ECIR and JCDL/SIGIR tackles issues related to academic search, at the crossroads between Information Retrieval, Natural Language Processing and Bibliometrics. In this overview paper, we summarize the past workshops, present the workshop topics for 2020 and reflect on some future steps for this workshop series.

========================================

Though computational linguistics (CL) dates back to the first efforts in machine translation in the mid 1950s, it is only in the last decade or so that it has had a substantial impact on literary studies through the statistical techniques of corpus linguistics and data mining (know as natural language processing, NLP). In this essay I briefly review the history of computational linguistics from its early days involving symbolic computing to current developments in NLP and set that in relationship to academic literary study. In particular, I discuss the deeply problematic struggle that literary study has had with the question of evaluation: What makes good literature? I argue that literary studies should own up to this tension and recognize a distinction between ethical criticism, which is explicitly concerned with values, and naturalist criticism, which sidesteps questions of value in favor of understanding how literature works in the mind and in culture. I then argue that the primary relationship between CL and NLP and literary studies should be through naturalist criticism. I conclude by discussing the relative roles of CL and NLP in a large-scale and long-term investigation of romantic love.

========================================

In recent years, because of the advancements in communication and networking technologies, mobile technologies have been developing at an unprecedented rate. mHealth, the use of mobile technologies in medicine, and the related research has also surged parallel to these technological advancements. Although there have been several attempts to review mHealth research through manual processes such as systematic reviews, the sheer magnitude of the number of studies published in recent years makes this task very challenging. The most recent developments in machine learning and text mining offer some potential solutions to address this challenge by allowing analyses of large volumes of texts through semi-automated processes. The objective of this study is to analyze the evolution of mHealth research by utilizing text-mining and natural language processing (NLP) analyses. The study sample included abstracts of 5,644 mHealth research articles, which were gathered from five academic search engines by using search terms such as mobile health, and mHealth. The analysis used the Text Explorer module of JMP Pro 13 and an iterative semi-automated process involving tokenizing, phrasing, and terming. After developing the document term matrix (DTM) analyses such as single value decomposition (SVD), topic, and hierarchical document clustering were performed, along with the topic-informed document clustering approach. The results were presented in the form of word-clouds and trend analyses. There were several major findings regarding research clusters and trends. First, our results confirmed time-dependent nature of terminology use in mHealth research. For example, in earlier versus recent years the use of terminology changed from "mobile phone" to "smartphone" and from "applications" to "apps". Second, ten clusters for mHealth research were identified including (I) Clinical Research on Lifestyle Management, (II) Community Health, (III) Literature Review, (IV) Medical Interventions, (V) Research Design, (VI) Infrastructure, (VII) Applications, (VIII) Research and Innovation in Health Technologies, (IX) Sensor-based Devices and Measurement Algorithms, (X) Survey-based Research. Third, the trend analyses indicated the infrastructure cluster as the highest percentage researched area until 2014. The Research and Innovation in Health Technologies cluster experienced the largest increase in numbers of publications in recent years, especially after 2014. This study is unique because it is the only known study utilizing text-mining analyses to reveal the streams and trends for mHealth research. The fast growth in mobile technologies is expected to lead to higher numbers of studies focusing on mHealth and its implications for various healthcare outcomes. Findings of this study can be utilized by researchers in identifying areas for future studies.

========================================

T-Know is a knowledge service system based on the constructed knowledge graph of Traditional Chinese Medicine (TCM). Using authorized and anonymized clinical records, medicine clinical guidelines, teaching materials, classic medical books, academic publications, etc., as data resources, the system extracts triples from free texts to build a TCM knowledge graph by our developed natural language processing methods. On the basis of the knowledge graph, a deep learning algorithm is implemented for single-round question understanding and multiple-round dialogue. In addition, the TCM knowledge graph also is used to support human-computer interactive knowledge retrieval by normalizing search keywords to medical terminology.

========================================

The Bibliometric-enhanced Information Retrieval workshop series (BIR) was launched at ECIR in 2014 [Mayr et al., 2014] and it was held at ECIR each year since then. This year we organized the 10th iteration of BIR as an all-virtual workshop with a peak of 97 participants. The workshop series at ECIR and JCDL/SIGIR tackles issues related to academic search, at the crossroads between Information Retrieval, Natural Language Processing and Bibliometrics. In this report, we summarize the past workshops, present the workshop topics for 2020 [Cabanac et al., 2020] and reflect on some future steps for this workshop series.

========================================

Wikipedia is the largest organized knowledge repository on the Web, increasingly employed by natural language processing and search tools. In this paper, we investigate the task of labeling Wikipedia pages with standard named entity tags, which can be used further by a range of information extraction and language processing tools. To train the classifiers, we manually annotated a small set of Wikipedia pages and then extrapolated the annotations using the Wikipedia category information to a much larger training set. We employed several distinct features for each page: bag-of-words, page structure, abstract, titles, and entity mentions. We report high accuracies for several of the classifiers built. As a result of this work, a Web service that classifies any Wikipedia page has been made available to the academic community.

========================================

. Machine Learning (ML) is becoming a more and more popular ﬁeld of knowledge, being a term known not only in the academic ﬁeld due to its successful applications to many real-world problems. The advent of Deep Learning and Big Data in the last decade has contributed to make it even more popular. Many companies, both large ones and SMEs, have created speciﬁc departments for ML and data analysis, being in fact their main activity in many cases. This current exploitation of ML should not mislead us; while it is a mature ﬁeld of knowledge, there is still room for many novel contributions, namely, a better understanding of the underlying Mathe-matics, proposal and tuning of algorithms suitable for new problems (e.g., Natural Language Processing), automation and optimization of the search of parameters, etc. Within this framework of new contributions to ML, Quantum Machine Learning (QML) has emerged strongly lately, speeding up ML calculations and providing alternative representations to existing approaches. This special session includes six high-quality papers dealing with some of the most relevant aspects of QML, including analysis of learning in quantum computing and quantum annealers, quantum versions of classical ML models –like neural networks or learning vector quantization–, and quantum learning approaches for measurement and control.

========================================

Breast cancer is the most common malignancy to spread to the orbit and periorbit, and the invasive lobular carcinoma (ILC) histologic subtype of breast cancer has been reported to form these ophthalmic metastases (OM) more frequently than invasive ductal carcinomas (IDC). We herein report our single academic institution experience with breast cancer OM with respect to anatomical presentation, histology (lobular vs. ductal), treatment, and survival. We employed the natural language processing platform, TIES (Text Information Extraction System), to search 2.3 million de-identified patient pathology and radiology records at our institution in order to identify patients with OM secondary to breast cancer. We then compared the resultant cohort, the “OM cohort,” to two other representative metastatic breast cancer patient (MBC) databases from our institution. Histological analysis of selected patients was performed. Our TIES search and manual refinement ultimately identified 28 patients who were diagnosed with breast cancer between 1995 and 2016 that subsequently developed OM. Median age at diagnosis was 54 (range 28–77) years of age. ER, PR, and HER2 status from the 28 patients with OM did not differ from other patients with MBC from our institution. The relative proportion of patients with ILC was significantly higher in the OM cohort (32.1%) than in other MBC patients in our institution (11.3%, p = 0.007). Median time to first OM in the OM cohort was 46.7 months, and OM were the second most frequent first metastases after bony metastases. After diagnosis of the first distant metastasis of any kind, median survival of patients with ILC (21.4 months) was significantly shorter than that of patients with IDC (55.3 months, p = 0.03). Nine patients developed bilateral OM. We observed a significant co-occurrence of OM and central nervous system metastases (p = 0.0053). The histological analysis revealed an interesting case in which the primary tumor was of a mixed ILC/IDC subtype, while only ILC was present in the OM. OM from breast cancer are illustrative of the difference in metastatic behavior of ILC versus IDC and should be considered when treating patients with ILC, especially in those with complaints of visual acuity changes.

========================================

Background Children with hemophilia frequently require long-term central venous access devices (CVADs) for regular infusion of factor products. Hemophilia patients are not immunocompromised, but the presence and use of CVADs are associated with infections including bacteremia. Currently, the utility of blood cultures in evaluation of the febrile hemophilia patient with an indwelling CVAD is unknown, nor is optimal empiric antibiotic use. Methods We performed a retrospective cross-sectional study of febrile immunocompetent hemophilia patients with CVADs presenting to a large academic urban pediatric emergency department from 1995 to 2017. We used a natural language processing electronic search, followed by manual chart review to construct the cohort. We analyzed rate of pathogen recovery from cultures of blood in subgroups of hemophilia patients, the pathogen profile, and the reported pathogen susceptibilities to ceftriaxone. Results Natural language processing electronic search identified 181 visits for fever among hemophilia patients with indwelling CVADs of which 147 cases from 44 unique patients met study criteria. Cultures of blood were positive in 56 (38%) of 147 patients (95% confidence interval, 30%–47%). Seventeen different organisms were isolated (10 pathogens and 7 possible pathogens) with Staphylococcus aureus and coagulase-negative Staphylococcus species as the most common. Thirty-four percent of isolates were reported as susceptible to ceftriaxone. Positive blood cultures were more common in cases involving patients with inhibitors (n = 71) versus those without (n = 76), odds ratio, 7.4 (95% confidence interval, 3.5–15.9). This was observed irrespective of hemophilia type. Conclusions Febrile immunocompetent hemophilia patients with indwelling CVADs have high rates of bacteremia. Empiric antimicrobial therapy should be targeted to anticipated pathogens and take into consideration local susceptibility patterns for Staphylococcus aureus.

========================================

This study sought to use ontology-based knowledge to identify patients with rare diseases and to estimate the frequency of those diseases in a large database of radiology reports. Natural language processing methods were applied to 12,377,743 narrarive-text radiology reports of 7,803,811 patients at an academic health system. Using knowledge from the Orphanet Rare Disease Ontology and Radiology Gamuts Ontology, 1,154 of 6,794 rare diseases (17.0%) were observed in a total of 237,840 patients (3.05%). Ninety of 2,129 diseases (4%) with known prevalence less than 1 per 1,000,000 were observed in the database, whereas 100 of 173 diseases (58%) with prevalence greater than 1 per 10,000 were observed; the difference was statistically significant (p < .00001). Automated ontology-based search of radiology reports can estimate the frequency of rare diseases, and those diseases with higher known prevalence were significantly more likely to appear in radiology reports.

========================================

News archives are an invaluable primary source for placing current events in historical context. But current search engine tools do a poor job at uncovering broad themes and narratives across documents. We present Rookie: a practical software system which uses natural language processing (NLP) to help readers, reporters and editors uncover broad stories in news archives. Unlike prior work, Rookie's design emerged from 18 months of iterative development in consultation with editors and computational journalists. This process lead to a dramatically different approach from previous academic systems with similar goals. Our efforts offer a generalizable case study for others building real-world journalism software using NLP.

========================================

Searching data is a natural behavior of humankind and is also a fundamental operation in both industrial and academic areas. There has been research into developing sophisticated methods and techniques for string searching. The common method is to make use of prefixes and suffixes to move through the text while searching a string query in a text. It suffers from high computational complexity since it has to repeatedly search each character in the string query sequentially. In this paper, we address the difficulty and propose a novel approach that enables a parallel search of the text without indexing the text or the query which is needed for sequential search. The proposed approach utilizes an XNOR operation in conjunction with the shift method to find the instance of a query. We validated it through experiments finding improvement against other methods.

========================================

Name disambiguation, which aims to identify multiple names which correspond to one person and same names which refer to different persons, is one of the most important basic problems in many areas such as natural language processing, information retrieval and digital libraries. Microsoft academic search data in KDD Cup 2013 Track 2 task brings one such challenge to the researchers in the knowledge discovery and data mining community. Besides the real-world and large-scale characteristic, the Track 2 task raises several challenges: (1) Consideration of both synonym and polysemy problems; (2) Existence of huge amount of noisy data with missing attributes; (3) Absence of labeled data that makes this challenge a cold start problem.
 In this paper, we describe our solution to Track 2 of KDD Cup 2013. The challenge of this track is author disambiguation, which aims at identifying whether authors are the same person by using academic publication data. We propose a multi-phase semi-supervised approach to deal with the challenge. First, we preprocess the dataset and generate features for models, then construct a coauthor-based network and employ community detection to accomplish first-phase disambiguation task, which handles the cold-start problem. Second, using results in first phase, we use support vector machine and various other models to utilize noisy data with missing attributes in the dataset. Further, we propose a self-taught procedure to solve ambiguity in coauthor information, boosting performance of results from other models. Finally, by blending results from different models, we finally achieves 6th place with 0.98717 mean F-score on public leaderboard and 7th place with 0.98651 mean F-score on private leaderboard.

========================================

The scholarly literature produced by human civilization will soon be considered small data, able to be portably conveyed by the network and carried on personal machines. This semi-structured text centric knowledge base is a focus of attention for scholars, as the wealth of facts, facets and connections in scholarly documents are large. Such machine analysis can derive insights that can inform policy makers, academic and industrial management, as well as scholars as authors themselves. There is another underserved community of scholarly document users that has been overlooked: the readers themselves. I call for the community to put more efforts towards supporting our own scholars (especially beginning scholars, new to the research process) with automation from information retrieval and natural language processing. Techniques that mine information from within the full text of a document could be used to introspect a digital library's materials, inferring better search metadata, improving scholarly document recommendation, and aiding the understanding of the text, figures, presentations and citations of our scholarly literature. Such an introspective digital library will enable scholars to assemble an understanding of other scholars' work more efficiently, and provide downstream machine reading applications with input for their analytics.

========================================

This paper presents a comprehensive NLP system by Melingo that has been recently developed for Arabic, based on Morfix™ - an operational formerly developed highly successful comprehensive Hebrew NLP system.The system discussed includes modules for morphological analysis, context sensitive lemmatization, vocalization, text-to-phoneme conversion, and syntactic-analysis-based prosody (intonation) model. It is employed in applications such as full text search, information retrieval, text categorization, textual data mining, online contextual dictionaries, filtering, and text-to-speech applications in the fields of telephony and accessibility and could serve as a handy accessory for non-fluent Arabic or Hebrew speakers.Modern Hebrew and Modern Standard Arabic share some unique Semitic linguistic characteristics. Yet up to now, the two languages have been handled separately in Natural Language Processing circles, both on the academic and on the applicative levels. This paper reviews the major similarities and the minor dissimilarities between Modern Hebrew and Modern Standard Arabic from the NLP standpoint, and emphasizes the benefit of developing and maintaining a unified system for both languages.

========================================

The WWW provides an efficient way to store and share information. Search engines and social bookmarking systems are important tools for web resource discovery. This study investigated three different indexing approaches applied to CiteULike — a social bookmarking system for tagging academic research papers. The indexing approaches here are known as: Tag only; Title with Abstract; and Tag, Title with Abstract. These three indexing approaches were evaluated using mean values of Normalized Discount Cumulative Gain (NDCG). The preliminary results illustrated that indexing using “Tag, Title, with Abstract” performed the best. The initial evaluation on our implementation implied that these designs might improve the accuracy and efficiency of web resource searching on social bookmarking system, not only in academics but also in other domains.

========================================

The NTU Corpus of Formosan Languages (http://corpus.linguistics.ntu.edu .tw) started as one of the sub-projects of the Multimedia Laboratory established by the Center for Information and ElectronicsTechnologies.The corpus integrates the professional and academic works of various departments and colleges1 at National Taiwan University, with a view to establish a standard for the creation of linguistic corpora databases through the application of information technology on linguistics research. The development of natural language processing techniques and dynamic web technology has generated wide interest in the construction of an integrated platform which enables people to submit, browse and search among collected texts against established corpora. For seriously endangered languages, corpus documentation plays a crucial role in preserving the precious linguistic and cultural data. The NTU Corpus of Formosan Languages is constructed for the purpose of preserving Austronesian Languages spoken in Taiwan, many of which are on the verge of extinction. It is the result of our effort in providing a multilingual on-line corpus with multimedia capability to meet the needs of both linguists and those interested. It features: (i) discourse-oriented documentation that goes beyond sentential level, (ii) retrieval with cross-referencing capability, (iii) ethnolinguistic notes preserving precious cultural information, and (iv) multi-media data with interoperability with different systems for the greatest interest of the users. To record faithfully discourse data collected, we incorporate in our corpus interactional linguistic features such as pauses, hesitations, fillers, repetitions, false starts etc.Approaching language from a functional approach, we believe linguistic features as such are non-trivial in that they may reveal cognitively and psychologically important details of language in use. Investigating spoken data in natural context makes it possible for us to explore issues beyond the scope offered by the traditional syntactic studies. For example, linguistic

========================================

Research in information retrieval is enjoying renewed interest by many different communities. Commercial retrieval systems, which in the past have concentrated on Boolean pat tern matching methodologies, are beginning to look into more sophist icated search methods, including complex stat istical and /o r natural language processing systems. This has spurred new interest in research in information retrieval in this community and also in the academic communities. Technology is being addit ionally pushed by the current emphasis on electronic communication, including digital libraries and the interlinking of oflices to allow ofrice automation. The availability of electronic text records in huge (and exponentially-growing) quantities, and the rapidly-expanding Internet access by potent ial users, is a third factor in promoting research. ARPA has contr ibuted to this increased interest by sponsoring a new test collection for information retrieval. The widespread availability of the T I P S T E R collection has allowed research on large-scale, real-world retrieval problems. This has not only opened up new areas of research tha t were not discovered using the smMler test collections, but has provided proof tha t the more complex retrieval systems do indeed scale up to handle realistic text collections. The first paper in this session, "Overview of the Second Text Retrieval Conference (TREC-2) ' , by Donna Harman, i l lustrates the use of this collection in a massive crosssystem evaluation. The TREC-2 conference, held in August of 1993, compared results from 31 different retrieval systems working with the T I P S T E R collection. These systems used many different approaches to retrieval, including manually constructed patterns, automatical ly constructed stat is t ical queries that were input to stat ist ical retrieval systems, and natural language approaches to information retrieval. The paper discusses the T I P S T E R test collection, the evaluation methods used in TREC, and the results from the conference. The next two papers in the session represent systems that appeared in TREG-2. The first of these papers discusses a mostly stat is t ical system and the second of these papers discusses a system using natural language processing techniques. The paper "Learning from Relevant Documents in Large Scale Routing Retrieval", by K.L. Kwok and L. Grunfeld, discusses experiments performed using a routing or filtering paradigm. This type of information retrieval assumes that users have a s tanding request for information, such as in an electronic dissemination service or an intelligence operation. There exists training information in the form of previously-seen documents considered relevant, and this training information is used to produce bet ter queries. This paper discusses in detail the problems of learning from fulltext relevant documents, which range in length from a short paragraph to many hundreds of pages. This problem is compounded by the availabili ty of large numbers of such relevant documents. Many experiments were performed to discover the opt imal method of selecting which (and what parts) of documents to use for training, and the results are given in the paper. The next paper, "Document Representat ion in Natural Language Text Retrieval", by Tomek Strzalkowski, discusses experiments performed using mostly the adhoc retrieval paradigm. In this case the documents are known in advance, but the information is requested on an "adhoc" basis. There is no training data , and systems are often required to deal with short user requests that might not map well onto the terminology used in the documents. One way around this problem is to automatical ly transform the user query into a linguistic s tructure that is expanded to bet ter map into the document collection. This paper presents a series of experiments in automatical ly locating useful lingulstic fragments of documents to match against such a modified user query. One of the main issues dealt with here is the correct term weighting for these fragments. Information retrieval is not l imited to the matching of textual material; two of the papers in the session deal with speech retrieval systems. The first of these papers describes a modification of t radi t ional information retrieval methods to handle speech, whereas the second paper uses t radi t ional speech recognition technology with information retrieval as the application. The paper, "Assessing the Retrieval Effectiveness of a Speech Retrieval System by Simulating Recognition Errors", by Peter Schauble and Ulrike Glavltsch, deals with retrieval of speech (speech "documents"). Their retrieval system uses phonetically motivated subword units as opposed to complete words for indexing of speech. The use of subwords as index terms means tha t the system can be used against either speech or text, and that techniques tradit ionally used in text retrieval can be modified for use with speech. The production of these subwords is dependent on current speech recognition technology, which is known to be error-prone. This paper presents some experiments using simulated speech recognition errors against well-known information retrieval test collections (textual) to see what effects these errors have on retrieval performance. The second of these papers, "Speech-Based Retrieval using Semantic Co-Occurrence Fil tering", by Julian Kupiec, Don Kimber, and Vijay Balasubramanian, uses a s tandard hidden Markov model as input to a text retrieval system. The issue in this paper is how to deal with the very large (generally unrestricted) vocabulary size that is normal for most text retrieval applications. Speech input using large vocabularies (and possibly many different speakers) is likely

========================================

Generative artificial intelligence tools have recently attracted a great deal of attention. This is because of their huge advantages, which include ease of usage, quick generation of answers to requests, and the human-like intelligence they possess. This paper presents a vivid comparative analysis of the top 9 generative artificial intelligence (AI) tools, namely ChatGPT, Perplexity AI, YouChat, ChatSonic, Google's Bard, Microsoft Bing Assistant, HuggingChat, Jasper AI, and Quora's Poe, paying attention to the Pros and Cons each of the AI tools presents. This comparative analysis shows that the generative AI tools have several Pros that outweigh the Cons. Further, we explore the transformative impact of generative AI in Natural Language Processing (NLP), focusing on its integration with search engines, privacy concerns, and ethical implications. A comparative analysis categorizes generative AI tools based on popularity and evaluates challenges in development, including data limitations and computational costs. The study highlights ethical considerations such as technology misuse and regulatory challenges. Additionally, we delved into AI Planning techniques in NLP, covering classical planning, probabilistic planning, hierarchical planning, temporal planning, knowledge-driven planning, and neural planning models. These planning approaches are vital in achieving specific goals in NLP tasks. In conclusion, we provide a concise overview of the current state of generative AI, including its challenges, ethical considerations, and potential applications, contributing to the academic discourse on human-computer interaction.  

========================================

Natural Language Processing (NLP) is poised to substantially influence the world. However, significant progress comes hand-in-hand with substantial risks. Addressing them requires broad engagement with various fields of study. Yet, little empirical work examines the state of such engagement (past or current). In this paper, we quantify the degree of influence between 23 fields of study and NLP (on each other). We analyzed ~77k NLP papers, ~3.1m citations from NLP papers to other papers, and ~1.8m citations from other papers to NLP papers. We show that, unlike most fields, the cross-field engagement of NLP, measured by our proposed Citation Field Diversity Index (CFDI), has declined from 0.58 in 1980 to 0.31 in 2022 (an all-time low). In addition, we find that NLP has grown more insular -- citing increasingly more NLP papers and having fewer papers that act as bridges between fields. NLP citations are dominated by computer science; Less than 8% of NLP citations are to linguistics, and less than 3% are to math and psychology. These findings underscore NLP's urgent need to reflect on its engagement with various fields.

========================================

Neural Architecture Search (NAS) is a promising and rapidly evolving research area. Training a large number of neural networks requires an exceptional amount of computational power, which makes NAS unreachable for those researchers who have limited or no access to high-performance clusters and supercomputers. A few benchmarks with precomputed neural architectures performances have been recently introduced to overcome this problem and ensure reproducible experiments. However, these benchmarks are only for the computer vision domain and, thus, are built from the image datasets and convolution-derived architectures. In this work, we step outside the computer vision domain by leveraging the language modeling task, which is the core of natural language processing (NLP). Our main contribution is as follows: we have provided search space of recurrent neural networks on the text datasets and trained 14k architectures within it; we have conducted both intrinsic and extrinsic evaluation of the trained models using datasets for semantic relatedness and language understanding evaluation; finally, we have tested several NAS algorithms to demonstrate how the precomputed results can be utilized. We consider that the benchmark will provide more reliable empirical findings in the community and stimulate progress in developing new NAS methods well suited for recurrent architectures.

========================================

The Information Retrieval (IR) process starts with the query entered by the user in the system. The IR system is mostly used in the search string in text utilities and web search engines. For IR algorithms to efficiently retrieve pertinent documents, the documents are usually converted into an appropriate representation. Query expansion and Natural Language Processing (NLP) are the most researched fields in Artificial Intelligence (AI). The query expansion process provides the additional information that is understood by the user in their query. This research study explores several NLP query expansion techniques that utilize word embedding methods for autonomous query expansion including deep learning, machine learning, and ontology approach. This research also discusses the importance of query expansion to improve search efficacy and provides a coherent view of NLP-based query expansion methods that leverage a range of data sources and methodologies.

========================================

Abstract Purpose Learner development and promotion rely heavily on narrative assessment comments, but narrative assessment quality is rarely evaluated in medical education. Educators have developed tools such as the Quality of Assessment for Learning (QuAL) tool to evaluate the quality of narrative assessment comments; however, scoring the comments generated in medical education assessment programs is time intensive. The authors developed a natural language processing (NLP) model for applying the QuAL score to narrative supervisor comments. Method Samples of 2,500 Entrustable Professional Activities assessments were randomly extracted and deidentified from the McMaster (1,250 comments) and Saskatchewan (1,250 comments) emergency medicine (EM) residency training programs during the 2019–2020 academic year. Comments were rated using the QuAL score by 25 EM faculty members and 25 EM residents. The results were used to develop and test an NLP model to predict the overall QuAL score and QuAL subscores. Results All 50 raters completed the rating exercise. Approximately 50% of the comments had perfect agreement on the QuAL score, with the remaining resolved by the study authors. Creating a meaningful suggestion for improvement was the key differentiator between high- and moderate-quality feedback. The overall QuAL model predicted the exact human-rated score or 1 point above or below it in 87% of instances. Overall model performance was excellent, especially regarding the subtasks on suggestions for improvement and the link between resident performance and improvement suggestions, which achieved 85% and 82% balanced accuracies, respectively. Conclusions This model could save considerable time for programs that want to rate the quality of supervisor comments, with the potential to automatically score a large volume of comments. This model could be used to provide faculty with real-time feedback or as a tool to quantify and track the quality of assessment comments at faculty, rotation, program, or institution levels.

========================================

Search engines (Google search, Bing search, etc.) have had great success over the past decade, promoting productivity in almost every area. Based on user inputs, search engines are able to present users with lists of related contents (links) and previews. More recently, high-level human-like responses combining various searched contents are being made possible due to recent advancements in large language models (LLM). However, oftentimes, users still find it still hard to quickly navigate to the contents they really look for and demand a better searching framework. For example, some users might waste time skimming through lots of technical details when they just hope to have an overview. We examine this user demand and believe a complexity-aware pipeline could greatly help with this inconvenience. More specifically, we propose a searching paradigm that analyzes results from standard search engines by their complexities first, and then present users with complexity-labeled contents through a new user interface design. Through this new searching paradigm, we aim to present users with more customized search results sorted by their complexity labels with consideration to user intent, whether that would be a high-level overview or a detailed technical inspection. This is done through utilizing state-of-the-art transformer models fine-tuned on our custom-made dataset and modified for our intent.

========================================

OBJECTIVES
Natural language processing (NLP) represents one of the adjunct technologies within artificial intelligence and machine learning, creating structure out of unstructured data. This study aims to assess the performance of employing NLP to identify and categorize unstructured data within the emergency medicine (EM) setting.


METHODS
We systematically searched publications related to EM research and NLP across databases including MEDLINE, Embase, Scopus, CENTRAL, and ProQuest Dissertations & Theses Global. Independent reviewers screened, reviewed, and evaluated article quality and bias. NLP usage was categorized into syndromic surveillance, radiologic interpretation, and identification of specific diseases/events/syndromes, with respective sensitivity analysis reported. Performance metrics for NLP usage were calculated and the overall area under the summary of receiver operating characteristic curve (SROC) was determined.


RESULTS
A total of 27 studies underwent meta-analysis. Findings indicated an overall mean sensitivity (recall) of 82%-87%, specificity of 95%, with the area under the SROC at 0.96 (95% CI 0.94-0.98). Optimal performance using NLP was observed in radiologic interpretation, demonstrating an overall mean sensitivity of 93% and specificity of 96%.


CONCLUSIONS
Our analysis revealed a generally favorable performance accuracy in using NLP within EM research, particularly in the realm of radiologic interpretation. Consequently, we advocate for the adoption of NLP-based research to augment EM health care management.

========================================

Natural language processing (NLP) is a promising tool for collecting data that are usually hard to obtain during extreme weather, like community response and infrastructure performance. Patterns and trends in abundant data sources such as weather reports, news articles, and social media may provide insights into potential impacts and early warnings of impending disasters. This paper reviews the peer-reviewed studies (journals and conference proceedings) that used NLP to assess extreme weather events, focusing on heavy rainfall events. The methodology searches four databases (ScienceDirect, Web of Science, Scopus, and IEEE Xplore) for articles published in English before June 2022. The preferred reporting items for systematic reviews and meta-analysis reviews and meta-analysis guidelines were followed to select and refine the search. The method led to the identification of thirty-five studies. In this study, hurricanes, typhoons, and flooding were considered. NLP models were implemented in information extraction, topic modeling, clustering, and classification. The findings show that NLP remains underutilized in studying extreme weather events. The review demonstrated that NLP could potentially improve the usefulness of social media platforms, newspapers, and other data sources that could improve weather event assessment. In addition, NLP could generate new information that should complement data from ground-based sensors, reducing monitoring costs. Key outcomes of NLP use include improved accuracy, increased public safety, improved data collection, and enhanced decision-making are identified in the study. On the other hand, researchers must overcome data inadequacy, inaccessibility, nonrepresentative and immature NLP approaches, and computing skill requirements to use NLP properly.